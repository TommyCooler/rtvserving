services:
  triton:
    image: hieupth/tritonserver:24.08
    container_name: rtvserving
    environment:
      HF_CONFIG_FILE: /hf.json
      HF_MODEL_REPO: /models
    volumes:
      - ./configs/hf.json:/hf.json
      - ./models:/models
      - ./hf.py:/hf.py

    tty: true
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 5s
      retries: 2
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    command: >
      bash -c "python3 -u /hf.py && tritonserver --model-repository=/models --model-config-name=tensorrt"