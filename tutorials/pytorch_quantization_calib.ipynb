{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/')\n",
    "query_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "ctx_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    for i, (encode_input) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        model(**encode_input)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "            \n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "#             print(F\"{name:40}: {module}\")\n",
    "    model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]')\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]')\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test = next(iter(calib_ctx_loader))\n",
    "for k, v in test.items():\n",
    "    print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# It is a bit slow since we collect histograms on CPU\n",
    "\n",
    "calib_batches = number_samples // batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    collect_stats(query_model, calib_query_loader, num_batches=calib_batches)\n",
    "    compute_amax(query_model, method=\"percentile\", percentile=99.99)\n",
    "    # compute_amax(query_model, method=\"mse\")\n",
    "    # compute_amax(query_model, method=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# It is a bit slow since we collect histograms on CPU\n",
    "\n",
    "calib_batches = number_samples // batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    collect_stats(ctx_model, calib_ctx_loader, num_batches=calib_batches)\n",
    "    compute_amax(ctx_model, method=\"percentile\", percentile=99.99)\n",
    "    # compute_amax(ctx_model, method=\"mse\")\n",
    "    # compute_amax(ctx_model, method=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_qry=None, \n",
    "    model_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, len(query_batch))\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, len(concate_passage))\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# becnhmark run onnx model\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        # self.inputs, self.outputs, self.bindings = self.allocate_buffers()\n",
    "        self.stream = cuda.Stream()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self, binding_shape):\n",
    "        # Allocate host and device buffers\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        for binding in self.engine:\n",
    "            # \n",
    "            binding_idx = self.engine.get_binding_index(binding)\n",
    "            # Set input shape based on image dimensions for inference\n",
    "            # conditional: binding is input\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                self.context.set_binding_shape(binding_idx, binding_shape)\n",
    "\n",
    "            size = trt.volume(self.context.get_binding_shape(binding_idx))\n",
    "            # print(\"size: \", size)\n",
    "            # print(\"batch_size: \", batch_size)\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "\n",
    "        return inputs, outputs, bindings\n",
    "       \n",
    "            \n",
    "    def __call__(self, inputs_id, attention_mask, token_type_ids, batch_size=2):\n",
    "\n",
    "        \n",
    "        x = np.array(inputs_id).astype(self.dtype)\n",
    "        y = np.array(attention_mask).astype(self.dtype)\n",
    "        z = np.array(token_type_ids).astype(self.dtype)\n",
    "\n",
    "\n",
    "        inputs, outputs, bindings = self.allocate_buffers(x.shape)\n",
    "    \n",
    "        # Transfer input data to the GPU.\n",
    "        # print(x.shape)\n",
    "        np.copyto(inputs[0].host,x.ravel())\n",
    "        np.copyto(inputs[1].host,y.ravel())\n",
    "        np.copyto(inputs[2].host,z.ravel())\n",
    "        \n",
    "        # after copy -> transfer to device, transer first will error duo to hold old value\n",
    "        for inp in inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "\n",
    "        # Run inference\n",
    "        self.context.execute_async_v2(bindings=bindings, stream_handle=self.stream.handle)\n",
    "        \n",
    "        # Transfer prediction output from the GPU.\n",
    "        for out in outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        \n",
    "        # Synchronize the stream\n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(batch_size,-1) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trt(texts, model, tokenizer, batch_size):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    # encoded_input = tokenizer(\n",
    "    #     texts, \n",
    "    #     padding=True, \n",
    "    #     truncation=True,\n",
    "    #     return_tensors='np'\n",
    "    # )\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model(\n",
    "        encoded_input['input_ids'],\n",
    "        encoded_input['attention_mask'],\n",
    "        encoded_input['token_type_ids'],\n",
    "        batch_size\n",
    "    )[0]\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -c \"import tensorrt; print(tensorrt.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "\n",
    "# trtversion 10.3.0\n",
    "\n",
    "trt_engine_qry_path = \"./mbert-retrieve-qry-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.engine\"\n",
    "trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"./mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "# trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "\n",
    "model_query = TrtModel(trt_engine_qry_path, max_batch_size=1, dtype=np.int32)\n",
    "model_ctx = TrtModel(trt_engine_ctx_path, max_batch_size=10, dtype=np.int32)\n",
    "tokenizer_qry = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-qry-onnx/\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-ctx-onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trt,\n",
    "    num_passages=10, \n",
    "    model_ctx=model_ctx,\n",
    "    model_qry=model_query, \n",
    "    tokenizer_ctx=tokenizer_ctx,\n",
    "    tokenizer_query=tokenizer_qry,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
