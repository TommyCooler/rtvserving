{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): QuantLinear(\n",
       "      in_features=768, out_features=768, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    raise ValueError(\"No GPU available\")\n",
    "# device = \"cpu\"\n",
    "query_model = BertModel.from_pretrained('../mbert-retrieve-qry-base/', add_pooling_layer=False)\n",
    "query_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): QuantLinear(\n",
       "      in_features=768, out_features=768, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "ctx_model = BertModel.from_pretrained('../mbert-retrieve-ctx-base/', add_pooling_layer=False)\n",
    "ctx_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('../mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('../mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    for i, (encode_input) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        model(**encode_input)\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "            \n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "#             print(F\"{name:40}: {module}\")\n",
    "    model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0202 08:26:12.571879 139961657057664 load.py:1444] Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "W0202 08:26:12.579268 139961657057664 cache.py:94] Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "W0202 08:26:13.274114 139961657057664 load.py:1444] Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "W0202 08:26:13.281836 139961657057664 cache.py:94] Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]')\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]')\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "test = next(iter(calib_ctx_loader))\n",
    "for k, v in test.items():\n",
    "    print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:10<00:00, 24.32it/s]\n",
      "W0202 08:26:24.218169 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.218636 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.218971 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.219309 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.219666 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.219982 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.220240 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.220518 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.220786 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.221031 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.221309 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.221579 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.221881 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.222164 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.222415 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.222694 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.222965 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.223212 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.223461 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.223744 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.224023 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.224275 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.224528 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.224804 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.225054 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.225296 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.225581 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.225840 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.226109 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.226363 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.226717 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.227021 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.227358 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.227627 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.227884 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.228163 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.228468 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.228725 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.231150 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.231451 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.231779 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.232065 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.232595 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.232839 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.233104 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.233363 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.233623 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.233893 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.234145 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.234406 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.234663 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.234913 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.235166 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.235431 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.235688 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.235933 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.236202 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.236454 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.236717 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.236956 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.237226 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.237474 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.237730 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.237981 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.238232 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.238496 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.238744 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.238995 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.239259 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.239507 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.239763 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.240020 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.240271 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.240521 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.240776 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.241035 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.241286 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.241524 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.241787 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.242053 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.242305 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.242563 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.242822 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.243081 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.243334 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.243583 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.243833 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.244088 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.244326 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.244581 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.244827 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.245082 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.245322 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.245592 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.245845 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.246094 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.249232 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.249489 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.249749 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.250006 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.250261 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.250514 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.250775 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.251016 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.251269 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.251534 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.251784 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.252032 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.252294 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.252667 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.252924 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.253180 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.253437 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.253736 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.254020 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.255309 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.255575 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.255906 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.256219 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.256534 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.256808 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.257066 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.257328 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.257578 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.257842 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.258090 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.258339 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.258605 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.258860 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.259107 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.259370 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.259623 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.259882 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.260125 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.260379 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.260637 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.260886 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.261129 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.261390 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.261644 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.261896 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.262146 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.262396 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.262650 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.262899 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:26:24.263158 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:26:24.265475 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.265741 139961657057664 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0202 08:26:24.266054 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.266626 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.266895 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.267415 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.267688 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.268216 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.268490 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.269001 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.269273 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.269794 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.270064 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.270598 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.270863 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.271370 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.271648 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.272156 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.272421 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.272944 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.273206 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.273731 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.274008 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.274527 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.274795 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.275321 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.275596 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.276100 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.276359 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.276875 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.277141 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.277670 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.277946 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.278463 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.278733 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.279243 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.279502 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.280032 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.280307 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.280807 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.281076 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.281581 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.281851 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.282377 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.282634 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.283153 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.283425 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.283942 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.284207 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.284736 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.285006 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.285516 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.285778 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.286300 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.286566 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.287095 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.287364 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.287872 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.288151 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.288661 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.288939 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.289481 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.289770 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.290342 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.290614 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.291160 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.291432 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.292057 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.292332 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.292849 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.293119 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.293633 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.293901 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.294445 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.294727 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.295240 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.295510 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.296025 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.296304 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.296818 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.297090 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.297603 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.297871 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.298423 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.298697 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.299233 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.299504 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.300022 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.300292 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.300809 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.301073 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.301612 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.301881 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.302418 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.302700 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.303221 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.303491 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.304018 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.304284 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.304800 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.305066 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.305580 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.305861 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.306394 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.306663 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.307183 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.307454 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.307980 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.308249 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.308771 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.309029 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.309547 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.309825 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.310338 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.310611 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.311147 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.311435 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.311954 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.312226 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.312749 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.313023 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.313554 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.313835 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.314355 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.314628 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.315134 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.315408 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.315934 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.316199 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.316721 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.316988 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.317507 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.317778 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.318308 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.318578 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.319102 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.319364 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.319883 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.320156 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.320672 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.320937 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.321453 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.321725 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:26:24.322248 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.322509 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:26:24.323039 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:26:24.323313 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n"
     ]
    }
   ],
   "source": [
    "# It is a bit slow since we collect histograms on CPU\n",
    "\n",
    "calib_batches = number_samples // batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    collect_stats(query_model, calib_query_loader, num_batches=calib_batches)\n",
    "    compute_amax(query_model, method=\"percentile\", percentile=99.99)\n",
    "    # compute_amax(query_model, method=\"mse\")\n",
    "    # compute_amax(query_model, method=\"entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\n",
      "W0202 08:27:15.113869 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.114361 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.114764 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.115107 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.115456 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.115826 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.116194 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.116595 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.116959 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.117311 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.117656 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.117936 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.118206 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.118469 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.118781 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.119089 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.119397 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.119705 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.120025 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.120336 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.120643 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.120910 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.121175 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.121432 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.121686 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.121949 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.122205 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.122452 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.122716 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.122959 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.123214 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.123456 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.123713 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.123982 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.124301 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.124611 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.124927 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.125230 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.125519 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.125779 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.126053 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.126309 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.126561 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.126813 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.127064 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.127376 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.127683 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.127985 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.128277 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.128561 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.128817 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.129069 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.129324 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.129547 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.129798 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.130075 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.130330 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.130589 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.130855 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.131118 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.131394 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.131640 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.135149 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.135399 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.135668 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.135915 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.136169 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.136429 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.136678 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.136932 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.137190 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.137439 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.137700 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.137968 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.138224 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.138485 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.138740 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.138977 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.139230 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.139488 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.139745 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.139981 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.140234 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.140487 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.140751 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.140987 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.141240 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.141496 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.141743 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.141986 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.142244 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.142496 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.142745 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.142980 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.143237 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.143489 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.143738 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.143980 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.144230 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.144481 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.144727 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.144968 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.145219 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.145476 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.145722 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.145977 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.146233 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.146493 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.146741 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.147001 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.147243 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.147500 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.147810 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.148129 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.148441 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.148745 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.149061 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.149369 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.149690 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.149955 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.150224 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.150481 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.150742 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.150997 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.151245 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.151486 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.151742 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.151998 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.152246 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.152488 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.152748 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.153005 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.153269 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.153665 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.153926 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.154189 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.154434 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.154681 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.154935 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.155189 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.155441 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.155693 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.155936 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.156313 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.156550 139961657057664 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0202 08:27:15.161060 139961657057664 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0202 08:27:15.161782 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.162112 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.162710 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.162994 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.163516 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.163789 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.164310 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.164574 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.165101 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.165371 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.165899 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.166169 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.166700 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.166981 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.167485 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.167751 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.168263 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.168522 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.169048 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.169326 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.169837 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.170102 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.170634 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.170898 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.171495 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.171763 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.172273 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.172535 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.173061 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.173329 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.173861 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.174139 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.174653 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.174924 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.175450 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.175723 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.176241 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.176506 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.177018 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.177294 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.177804 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.178084 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.178594 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.178864 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.179372 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.179631 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.180156 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.180426 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.180949 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.181216 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.181725 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.182001 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.182535 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.182809 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.183332 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.183597 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.184119 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.184384 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.184906 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.185178 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.185689 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.185970 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.186483 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.186753 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.187277 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.187538 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.188065 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.188341 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.188852 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.189119 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.189649 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.189925 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.190448 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.190715 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.191233 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.191498 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.192012 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.192284 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.192795 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.193058 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.193578 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.193844 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.194382 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.194651 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.195172 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.195447 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.195950 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.196223 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.196752 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.197024 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.197531 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.197791 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.198323 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.198591 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.199123 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.199390 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.199911 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.200178 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.200704 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.200977 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.201515 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.201788 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.202332 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.202595 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.203226 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.203488 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.204013 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.204280 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.204800 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.205072 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.205586 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.205880 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.206429 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.206721 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.207239 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.207499 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.208028 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.208298 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.208817 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.209089 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.209618 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.209901 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.210449 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.210736 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.211281 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.211559 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.212093 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.212358 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.212889 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.213164 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.213685 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.213968 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.214513 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.214782 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.215291 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.215552 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.216071 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.216341 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.216888 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.217157 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.217673 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.218049 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "W0202 08:27:15.218538 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.218834 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "W0202 08:27:15.219372 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0202 08:27:15.219733 139961657057664 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([768, 1]).\n"
     ]
    }
   ],
   "source": [
    "# It is a bit slow since we collect histograms on CPU\n",
    "\n",
    "calib_batches = number_samples // batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    collect_stats(ctx_model, calib_ctx_loader, num_batches=calib_batches)\n",
    "    compute_amax(ctx_model, method=\"percentile\", percentile=99.99)\n",
    "    # compute_amax(ctx_model, method=\"mse\")\n",
    "    # compute_amax(ctx_model, method=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "qry_model_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/qry_model_calib_percentile.pth\"\n",
    "ctx_model_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/ctx_model_calib_percentile.pth\"\n",
    "if os.path.exists(qry_model_path):\n",
    "    os.remove(qry_model_path)\n",
    "if os.path.exists(ctx_model_path):\n",
    "    os.remove(ctx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(query_model.state_dict(), qry_model_path)\n",
    "torch.save(ctx_model.state_dict(), ctx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_model_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/qry_model_calib_percentile.pth\"\n",
    "ctx_model_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/ctx_model_calib_percentile.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.4014 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1138, 0.5235](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.4014 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1064, 0.4855](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.4014 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0633, 0.3268](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.1263 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0767, 0.6192](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=18.0490 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0618, 1.0209](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.5476 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1132, 3.6558](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=7.0389 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1026, 0.3948](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=7.0389 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1102, 0.3941](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=7.0389 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0789, 0.1809](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.0485 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0753, 0.4708](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=26.2144 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0721, 0.7913](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.8820 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1331, 5.8223](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.2242 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0950, 0.3201](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.2242 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0966, 0.3955](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.2242 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0776, 0.1932](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.0635 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0760, 0.4410](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=37.7410 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0799, 0.6610](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.9229 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1111, 5.0647](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.5862 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1159, 0.3415](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.5862 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1140, 0.3253](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.5862 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0749, 0.2304](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.6559 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0480, 0.3765](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=40.5336 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0790, 0.6684](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=4.2146 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1163, 6.5838](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.0779 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1192, 0.3599](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.0779 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1144, 0.3857](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.0779 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0854, 0.3264](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.9247 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0677, 0.2323](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=39.6074 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0730, 0.5920](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=4.6296 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1179, 14.3991](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.3167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1129, 0.4242](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.3167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1160, 0.4092](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.3167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0674, 0.2751](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6684 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0592, 0.2464](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=32.2734 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0753, 0.4768](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.3069 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1068, 15.5112](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6118 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1070, 0.3139](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6118 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1059, 0.3443](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6118 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0660, 0.2483](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6739 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0682, 0.2620](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=21.4085 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0730, 0.3180](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.4330 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1096, 5.4419](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.0206 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1118, 0.2443](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.0206 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1095, 0.3582](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.0206 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0766, 0.2476](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.7163 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0634, 0.2696](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.7268 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0720, 0.4793](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.2213 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1056, 2.2813](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3076 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1131, 0.2737](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3076 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1138, 0.3686](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3076 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0813, 0.2266](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.5529 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0690, 0.2532](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=27.3717 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0720, 1.1037](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.2956 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1208, 3.6263](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.9387 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1096, 0.3460](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.9387 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1029, 0.4259](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.9387 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0743, 0.2084](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.8036 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0663, 0.6908](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=29.2255 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0779, 1.3988](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.2448 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1252, 5.2575](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.7167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1017, 0.2699](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.7167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1121, 0.2694](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.7167 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0883, 0.2666](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6557 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0695, 0.5556](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=28.0331 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0848, 1.2979](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.9679 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1223, 7.2757](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=24.1196 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1077, 0.2588](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=24.1196 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1126, 0.3121](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=24.1196 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1077, 0.2662](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.1588 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0830, 0.5565](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=24.0384 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0690, 0.4387](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.1072 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1081, 1.4602](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): QuantLinear(\n",
       "      in_features=768, out_features=768, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.1684 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0519, 0.0983](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "import pytorch_quantization.utils\n",
    "# export onnx\n",
    "# load the calibrated model\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "query_dummy_model = BertModel.from_pretrained('../mbert-retrieve-qry-base/', add_pooling_layer=False)\n",
    "ctx_dummy_model = BertModel.from_pretrained('../mbert-retrieve-ctx-base/', add_pooling_layer=False)\n",
    "query_dummy_model.load_state_dict(torch.load(qry_model_path, map_location=\"cpu\"))\n",
    "ctx_dummy_model.load_state_dict(torch.load(ctx_model_path, map_location=\"cpu\"))\n",
    "query_dummy_model.to(device)\n",
    "ctx_dummy_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = {\n",
    "#     'input_ids': torch.randint(0, 100, (1, 512)).to(device),\n",
    "#     'attention_mask': torch.randint(0, 1, (1, 512)).to(device),\n",
    "#     'token_type_ids': torch.randint(0, 1, (1, 512)).to(device),\n",
    "# }\n",
    "\n",
    "# traced_model = torch.jit.trace(query_dummy_model.to(device), (\n",
    "#     dummy_input['input_ids'].to(device), \n",
    "#     dummy_input['attention_mask'].to(device), \n",
    "#     dummy_input['token_type_ids'].to(device)\n",
    "# ), strict=False)\n",
    "# traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = {\n",
    "    'input_ids': torch.randint(0, 100, (1, 512)).to(device),\n",
    "    'attention_mask': torch.randint(0, 1, (1, 512)).to(device),\n",
    "    'token_type_ids': torch.randint(0, 1, (1, 512)).to(device),\n",
    "}\n",
    "\n",
    "input_names = [ \"input_ids\", \"attention_mask\", \"token_type_ids\" ]\n",
    "output_names = [ \"last_hidden_state\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:378: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:381: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:391: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:397: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n"
     ]
    }
   ],
   "source": [
    "quant_nn.TensorQuantizer._enable_onnx_export = True\n",
    "# enable_onnx_checker needs to be disabled. See notes below.\n",
    "torch.onnx.export(\n",
    "    ctx_dummy_model, \n",
    "    dummy_input, \n",
    "    \"../outputs/onnx/mbert-retrieve-ctx-onnx/ctx_quant_percential_calib.onnx\",\n",
    "    verbose=True, \n",
    "    opset_version=17,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch', 1: 'sequence'},\n",
    "        'token_type_ids': {0: 'batch', 1: 'sequence'},\n",
    "        'last_hidden_state': {0: 'batch', 1: 'sequence'}\n",
    "    },\n",
    ")\n",
    "quant_nn.TensorQuantizer._enable_onnx_export = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_nn.TensorQuantizer._enable_onnx_export = True\n",
    "torch.onnx.export(\n",
    "    query_dummy_model, \n",
    "    dummy_input, \n",
    "    \"../outputs/onnx/mbert-retrieve-qry-onnx/qry_quant_percential_calib.onnx\",\n",
    "    verbose=True, \n",
    "    opset_version=17,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch', 1: 'sequence'},\n",
    "        'token_type_ids': {0: 'batch', 1: 'sequence'},\n",
    "        'last_hidden_state': {0: 'batch', 1: 'sequence'}\n",
    "    },\n",
    ")\n",
    "quant_nn.TensorQuantizer._enable_onnx_export = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.3772 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1111, 0.5229](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.3772 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1067, 0.4843](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.3772 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0607, 0.3249](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.5392 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0770, 0.6147](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=13.0929 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0639, 1.0224](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=4.0503 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1137, 3.6632](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.8419 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1026, 0.3953](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.8419 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1099, 0.3933](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=6.8419 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0786, 0.1795](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.9042 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0716, 0.4689](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=28.8338 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0697, 0.7964](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.9619 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1315, 5.8228](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3307 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0998, 0.3168](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3307 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0947, 0.3951](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.3307 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0777, 0.1958](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6885 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0737, 0.4407](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=41.9339 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0789, 0.6601](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.3790 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1113, 5.0700](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1533 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1132, 0.3390](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1533 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1137, 0.3255](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1533 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0760, 0.2306](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.5124 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0449, 0.3754](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=42.8265 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0769, 0.6677](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.9338 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1142, 6.5891](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.3868 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1168, 0.3584](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.3868 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1159, 0.3866](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.3868 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0893, 0.3268](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.5561 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0685, 0.2318](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=41.2348 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0710, 0.5919](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.1121 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1193, 14.3994](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.5458 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1143, 0.4216](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.5458 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1193, 0.4084](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=16.5458 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0683, 0.2704](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6627 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0597, 0.2490](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=33.2351 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0755, 0.4743](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.7922 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1061, 15.5175](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6798 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1050, 0.3147](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6798 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1089, 0.3451](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=17.6798 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0656, 0.2526](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.5420 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0694, 0.2601](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.2529 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0711, 0.3166](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.8080 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1101, 5.4441](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.9499 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1155, 0.2501](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.9499 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1136, 0.3567](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.9499 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0785, 0.2462](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.5190 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0647, 0.2690](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=30.0183 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0714, 0.4817](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.8199 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1043, 2.2904](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.7580 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1134, 0.2748](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.7580 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1121, 0.3700](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.7580 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0803, 0.2251](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.3901 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0697, 0.2553](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=31.8193 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0725, 1.1108](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.0175 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1206, 3.6296](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1832 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1080, 0.3455](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1832 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1021, 0.4286](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=15.1832 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0739, 0.2066](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.7270 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0644, 0.6923](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=33.3046 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0777, 1.3980](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.8081 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1245, 5.2646](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=18.9957 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1013, 0.2683](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=18.9957 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1112, 0.2698](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=18.9957 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0864, 0.2650](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.6491 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0706, 0.5544](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=30.8792 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0862, 1.2994](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.7506 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1222, 7.2791](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.7743 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1084, 0.2602](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (key): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.7743 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1144, 0.3111](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (value): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.7743 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1097, 0.2654](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): QuantLinear(\n",
       "              in_features=768, out_features=768, bias=True\n",
       "              (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.0317 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "              (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0850, 0.5609](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=25.9400 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0657, 0.4374](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): QuantLinear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=1.3318 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "            (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1055, 1.4667](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): QuantLinear(\n",
       "      in_features=768, out_features=768, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.4145 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0516, 0.0941](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert tensorrt\n",
    "use `tritonserver` image (experiment in version 24.01 or 24.08)\n",
    "```bash\n",
    "docker run --gpus all -it --rm -v /home/tiennv/hungnq/rtvserving/outputs/onnx:/onnx nvcr.io/nvidia/tensorrt:24.08-py3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quantize calibration` for query & ctx onnx -> trt with flags fp32 & int8 & dynamic_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/qry_quant_percential_calib.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_calib_percential_fp32_int8_dynamic_shape.plan \\\n",
    "  --shapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "  --optShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/ctx_quant_percential_calib.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_int8_dynamic_shape.plan \\\n",
    "  --shapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "  --optShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quantize calibration` for query & ctx onnx -> trt with flags fp32 & dynamic_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/qry_quant_percential_calib.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_calib_percential_fp32_dynamic_shape.plan \\\n",
    "  --shapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "  --optShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/ctx_quant_percential_calib.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_dynamic_shape.plan \\\n",
    "  --shapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --minShapes=input_ids:1x128,attention_mask:1x128,token_type_ids:1x128 \\\n",
    "  --optShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_qry=None, \n",
    "    model_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, len(query_batch))\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, len(concate_passage))\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# becnhmark run onnx model\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        # self.inputs, self.outputs, self.bindings = self.allocate_buffers()\n",
    "        self.stream = cuda.Stream()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self, binding_shape):\n",
    "        # Allocate host and device buffers\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        for binding in self.engine:\n",
    "            # \n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                self.context.set_input_shape(binding, binding_shape)\n",
    "                \n",
    "            # print(\"binding: \", binding)\n",
    "            size = trt.volume(self.context.get_tensor_shape(binding))\n",
    "            # print(\"size: \", size)\n",
    "            # print(\"batch_size: \", self.context.get_tensor_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_tensor_dtype(binding))\n",
    "            # print(\"dtype: \", dtype)\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            # if self.engine.binding_is_input(binding):\n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "\n",
    "        return inputs, outputs, bindings\n",
    "       \n",
    "            \n",
    "    def __call__(self, inputs_id, attention_mask, token_type_ids, batch_size=2):\n",
    "\n",
    "        \n",
    "        x = np.array(inputs_id).astype(self.dtype)\n",
    "        y = np.array(attention_mask).astype(self.dtype)\n",
    "        z = np.array(token_type_ids).astype(self.dtype)\n",
    "\n",
    "\n",
    "        inputs, outputs, bindings = self.allocate_buffers(x.shape)\n",
    "    \n",
    "        # Transfer input data to the GPU.\n",
    "        # print(x.shape)\n",
    "        np.copyto(inputs[0].host,x.ravel())\n",
    "        np.copyto(inputs[1].host,y.ravel())\n",
    "        np.copyto(inputs[2].host,z.ravel())\n",
    "        \n",
    "        # after copy -> transfer to device, transer first will error duo to hold old value\n",
    "        for inp in inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "\n",
    "        # Run inference\n",
    "        self.context.execute_v2(bindings=bindings)\n",
    "        \n",
    "        # Transfer prediction output from the GPU.\n",
    "        for out in outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        \n",
    "        # Synchronize the stream\n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(batch_size,-1) for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trt(texts, model, tokenizer, batch_size):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    # encoded_input = tokenizer(\n",
    "    #     texts, \n",
    "    #     padding=True, \n",
    "    #     truncation=True,\n",
    "    #     return_tensors='np'\n",
    "    # )\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model(\n",
    "        encoded_input['input_ids'],\n",
    "        encoded_input['attention_mask'],\n",
    "        encoded_input['token_type_ids'],\n",
    "        batch_size\n",
    "    )[0]\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorrt; print(tensorrt.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## from `onnx_dynamic_quantization.ipynb`\n",
    "# trt_engine_qry_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.plan\"\n",
    "# trt_engine_ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.plan\"\n",
    "\n",
    "# trt_engine_qry_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.plan\"\n",
    "# trt_engine_ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.plan\"\n",
    "\n",
    "# ## above convert\n",
    "trt_engine_qry_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model_calib_percential_fp32_dynamic_shape.plan\"\n",
    "trt_engine_ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_dynamic_shape.plan\"\n",
    "\n",
    "# trt_engine_qry_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model_calib_percential_fp32_int8_dynamic_shape.plan\"\n",
    "# trt_engine_ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_int8_dynamic_shape.plan\"\n",
    "\n",
    "\n",
    "model_query = TrtModel(trt_engine_qry_path, max_batch_size=1, dtype=np.int32)\n",
    "model_ctx = TrtModel(trt_engine_ctx_path, max_batch_size=10, dtype=np.int32)\n",
    "tokenizer_qry = AutoTokenizer.from_pretrained(\"../outputs/onnx/mbert-retrieve-qry-onnx/\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"../outputs/onnx/mbert-retrieve-ctx-onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_667609/162317057.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 83.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798\n",
      "Time Query Run: 0.0018478615283966065\n",
      "Time Passage Run: 0.007641497373580932\n",
      "Time Query Total: 0.0022516303062438967\n",
      "Time Passage Total: 0.009141062021255492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trt,\n",
    "    num_passages=10, \n",
    "    model_ctx=model_ctx,\n",
    "    model_qry=model_query, \n",
    "    tokenizer_ctx=tokenizer_ctx,\n",
    "    tokenizer_query=tokenizer_qry,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
