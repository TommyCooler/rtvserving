{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert using docker \n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt-llm version 0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" trtllm_tritonserver bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. comment these following line:\n",
    "    - line 129 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\n",
    "    ```\n",
    "    Comment\n",
    "        if pynvml.__version__ < '11.5.0' or driver_version < '526':\n",
    "            logger.warning(\n",
    "                f'Found pynvml=={pynvml.__version__} and cuda driver version '\n",
    "                f'{driver_version}. Please use pynvml>=11.5.0 and cuda '\n",
    "                f'driver>=526 to get accurate memory usage.')\n",
    "            # Support legacy pynvml. Note that an old API could return\n",
    "            # wrong GPU memory usage.\n",
    "            _device_get_memory_info_fn = pynvml.nvmlDeviceGetMemoryInfo\n",
    "        else:\n",
    "    ```\n",
    "    => Only reserve after `else`, then fix index and save file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### custom mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error:\n",
    "[02/02/2025-04:25:43] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
    "[02/02/2025-04:25:43] [TRT] [E] IBuilder::buildSerializedNetwork: Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: BertRetriever/layers/0/attention/__add___L321/elementwise_binary_L2855/ELEMENTWISE_SUM_0: dimensions not compatible for elementwise. Broadcast has incompatible dimensions: 128 != 256 && 128 != 1 && 256 != 1.)\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/third_party/TensorRT-LLM/examples/bert/build_retrieve.py\", line 273, in <module>\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py\", line 204, in decorated\n",
    "    return f(*args, **kwargs)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 411, in build_engine\n",
    "    assert engine is not None, 'Engine building failed, please check the error log.'\n",
    "AssertionError: Engine building failed, please check the error log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (206555486.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 build_retrieve.py \\\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# add new file for customizing input \n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/bert-retrieve-qry-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32 + bert_attetion_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16 + bert_attetion_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt_llm v0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" heronq02/trtllm-tritonserver:24.12 bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 1 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/home/tiennv/hungnq/rtvserving/mbert-retrieve-qry-base\"   \\\n",
    "    --output_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 1 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/home/tiennv/hungnq/rtvserving/mbert-retrieve-ctx-base\"   \\\n",
    "    --output_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 8 \\\n",
    "    --max_input_len 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung-10_7/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-13 19:56:39.630156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739451399.642938 2167575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739451399.646919 2167575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 19:56:39.660610: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# isort: off\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "# isort: on\n",
    "\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import logger\n",
    "from tensorrt_llm.runtime import Session, TensorInfo\n",
    "\n",
    "# from build import get_engine_name  # isort:skip\n",
    "def get_engine_name(model, dtype, tp_size, rank):\n",
    "    return '{}_{}_tp{}_rank{}.plan'.format(model, dtype, tp_size, rank)\n",
    "\n",
    "def trt_dtype_to_torch(dtype):\n",
    "    if dtype == trt.float16:\n",
    "        return torch.float16\n",
    "    elif dtype == trt.float32:\n",
    "        return torch.float32\n",
    "    elif dtype == trt.int32:\n",
    "        return torch.int32\n",
    "    else:\n",
    "        raise TypeError(\"%s is not supported\" % dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt_llm\n",
    "\n",
    "tensorrt_llm.logger.set_level(\"info\")\n",
    "\n",
    "def get_model_config(config_path):\n",
    "    \n",
    "    config_path = os.path.join(config_path, 'config.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # V0.16.0 add builder \n",
    "    # assert config[\"plugin_config\"][\"remove_input_padding\"] == False, \\\\\n",
    "    # assert config['build_config'][\"plugin_config\"][\"remove_input_padding\"] == False, \\\n",
    "    #     \"Please refer to run_remove_input_padding.py for running BERT models with remove_input_padding enabled\"\n",
    "    return config\n",
    "\n",
    "def get_session(config_path):\n",
    "    config = get_model_config(config_path)\n",
    "    # world_size = config['builder_config']['tensor_parallel']\n",
    "    world_size = config['build_config']['auto_parallel_config']['world_size']\n",
    "    assert world_size == tensorrt_llm.mpi_world_size(), \\\n",
    "        f'Engine world size ({world_size}) != Runtime world size ({tensorrt_llm.mpi_world_size()})'\n",
    "\n",
    "    runtime_rank = tensorrt_llm.mpi_rank() if world_size > 1 else 0\n",
    "    runtime_mapping = tensorrt_llm.Mapping(world_size,\n",
    "                                            runtime_rank,\n",
    "                                            tp_size=world_size)\n",
    "    \n",
    "    torch.cuda.set_device(runtime_rank % runtime_mapping.gpus_per_node)\n",
    "    # dtype = config['builder_config']['precision']\n",
    "    # model_name = config['builder_config']['name']\n",
    "    # serialize_path = get_engine_name(\n",
    "    #     model_name, \n",
    "    #     dtype, world_size, runtime_rank\n",
    "    # )\n",
    "    serialize_path = 'rank0.engine'\n",
    "    serialize_path = os.path.join(config_path, serialize_path)\n",
    "    print(serialize_path)\n",
    "\n",
    "    logger.info(f'Loading engine from {serialize_path}')\n",
    "    with open(serialize_path, 'rb') as f:\n",
    "        engine_buffer = f.read()\n",
    "\n",
    "    logger.info(f'Creating session from engine')\n",
    "    return Session.from_serialized_engine(engine_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def process_input(input_ids_list: List[torch.Tensor],\n",
    "                  token_type_ids_list: List[torch.Tensor],\n",
    "                  is_roberta=False,\n",
    "                  padding_idx=1):\n",
    "    input_lengths = []\n",
    "    position_ids_list = []\n",
    "    max_input_length = 0\n",
    "    for i, input_ids in enumerate(input_ids_list):\n",
    "        input_len = len(input_ids)\n",
    "        assert input_len == len(token_type_ids_list[i]), f\"sample {i}: len(input_ids)={len(input_ids)}, \" \\\n",
    "                                                         f\"len(token_type_ids)={len(token_type_ids_list[i])}, not equal\"\n",
    "        input_lengths.append(input_len)\n",
    "        position_ids = torch.arange(0, input_len, dtype=torch.int32)\n",
    "        if is_roberta:\n",
    "            position_ids = position_ids + 1 + padding_idx\n",
    "\n",
    "        position_ids_list.append(position_ids)\n",
    "        max_input_length = max(max_input_length, input_len)\n",
    "\n",
    "    # [num_tokens]\n",
    "    input_ids = torch.concat(input_ids_list).int().cuda()\n",
    "    token_type_ids = torch.concat(token_type_ids_list).int().cuda()\n",
    "    position_ids = torch.concat(position_ids_list).int().cuda()\n",
    "\n",
    "    input_lengths = torch.tensor(input_lengths).int().cuda()  # [batch_size]\n",
    "    max_input_length = torch.empty((max_input_length, )).int().cuda()\n",
    "    return input_ids, input_lengths, token_type_ids, position_ids, max_input_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def prepare_inputs(texts: list, tokenizer: AutoTokenizer, config) -> dict:\n",
    "    #\n",
    "    remove_padding = config['build_config']['plugin_config'][\n",
    "        'remove_input_padding'\n",
    "    ]\n",
    "    model_name = config['pretrained_config']['architecture']\n",
    "\n",
    "    # Roberta doesn't have token_type_ids, use all zeros to replace\n",
    "    is_roberta = \"Roberta\" in model_name\n",
    "\n",
    "    if remove_padding:\n",
    "        input_ids_list = [\n",
    "            torch.tensor(ids).int().cuda() \\\n",
    "            for ids in inputs_without_padding['input_ids']\n",
    "        ]\n",
    "        # attention_mask_list = inputs_without_padding['attention_mask'],\n",
    "        token_type_ids_list = [\n",
    "            torch.tensor(ids).int().cuda() \\\n",
    "            for ids in inputs_without_padding['token_type_ids']\n",
    "        ]\n",
    "        \n",
    "        input_ids, input_lengths, token_type_ids, position_ids, max_input_length = process_input(\n",
    "            input_ids_list=input_ids_list,\n",
    "            token_type_ids_list=token_type_ids_list,\n",
    "            is_roberta=is_roberta,\n",
    "            padding_idx=config['pretrained_config']['pad_token_id']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        #NOTE:Padding: pad to longest seq len\n",
    "        inputs_with_padding = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        inputs_without_padding = tokenizer(texts)\n",
    "\n",
    "        input_ids = torch.tensor(inputs_with_padding['input_ids']).int().cuda()\n",
    "        input_lengths = [len(x) for x in inputs_without_padding['input_ids']]\n",
    "        input_lengths = torch.tensor(input_lengths,\n",
    "                                     device=input_ids.device,\n",
    "                                     dtype=torch.int32)\n",
    "        \n",
    "        # attention_mask = torch.tensor(inputs_with_padding['attention_mask'],\n",
    "        #                               device=input_ids.device,\n",
    "        #                               dtype=torch.int32)\n",
    "        if is_roberta:\n",
    "            token_type_ids = torch.zeros_like(torch.tensor(\n",
    "                inputs_with_padding['input_ids']),\n",
    "                                              device=input_ids.device,\n",
    "                                              dtype=torch.int32)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor(inputs_with_padding['token_type_ids'],\n",
    "                                          device=input_ids.device,\n",
    "                                          dtype=torch.int32)\n",
    "            \n",
    "    # NOTE: TRT-LLM perform inference\n",
    "    if remove_padding:\n",
    "        # NOTE: Remove padding:\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"input_lengths\": input_lengths,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "    else:\n",
    "        #NOTE: Padding:\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'input_lengths': input_lengths,\n",
    "            'token_type_ids': token_type_ids,\n",
    "        }\n",
    "    return inputs\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trtllm(texts, model, tokenizer, config, stream):\n",
    "    # check if tokenize length is min 128\n",
    "    inputs_model = prepare_inputs(texts, tokenizer, config)\n",
    "\n",
    "    if config['build_config']['plugin_config']['remove_input_padding']:\n",
    "        output_info = model.infer_shapes([\n",
    "            TensorInfo(\"input_ids\", trt.DataType.INT32, inputs_model['input_ids'].shape),\n",
    "            TensorInfo(\"input_lengths\", trt.DataType.INT32, inputs_model['input_lengths'].shape),\n",
    "            TensorInfo(\"token_type_ids\", trt.DataType.INT32, inputs_model['token_type_ids'].shape),\n",
    "            TensorInfo(\"position_ids\", trt.DataType.INT32, inputs_model['position_ids'].shape),\n",
    "            TensorInfo(\"max_input_length\", trt.DataType.INT32, inputs_model['max_input_length'].shape)\n",
    "        ])\n",
    "    else:\n",
    "        #NOTE: Padding:\n",
    "        output_info = model.infer_shapes([\n",
    "            TensorInfo(\"input_ids\", trt.DataType.INT32, inputs_model['input_ids'].shape),\n",
    "            TensorInfo(\"input_lengths\", trt.DataType.INT32, inputs_model['input_lengths'].shape),\n",
    "            TensorInfo(\"token_type_ids\", trt.DataType.INT32, inputs_model['token_type_ids'].shape),\n",
    "        ])\n",
    "    \n",
    "    outputs = {\n",
    "        t.name: torch.empty(\n",
    "            tuple(t.shape),\n",
    "            dtype=trt_dtype_to_torch(t.dtype),\n",
    "            device='cuda'\n",
    "        )\n",
    "        for t in output_info\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    ok = model.run(inputs_model, outputs, stream)\n",
    "    assert ok, \"Runtime execution failed\"\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    embeddings = outputs['hidden_states']\n",
    "    return embeddings.reshape(len(texts), -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    qry_model_dir=None, \n",
    "    ctx_model_dir=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    # assert model_ctx is not None, \"model_ctx is required\"\n",
    "    # assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 5, \"encode_fn must have 5 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    model_qry = get_session(qry_model_dir)\n",
    "    model_ctx = get_session(ctx_model_dir)\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        query_config = get_model_config(qry_model_dir)\n",
    "        stream = torch.cuda.current_stream().cuda_stream\n",
    "        # print(model_qry)\n",
    "        # print(tokenizer_query)\n",
    "        # print(query_config)\n",
    "\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, config=query_config, stream=stream)\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        passage_config = get_model_config(ctx_model_dir)\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        # print(model_ctx)\n",
    "        # print(tokenizer_ctx)\n",
    "        # print(passage_config)\n",
    "        \n",
    "        stream2 = torch.cuda.current_stream().cuda_stream\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, config=passage_config, stream=stream2)\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        # print(scores)\n",
    "        if scores.argmax(dim=1).detach().cpu().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run with attention plugin\n",
    "use code run of tensorrtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_engine_dir = \"../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"\n",
    "ctx_engine_dir = \"../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:56:42] [TRT-LLM] [I] Loading engine from ../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2167575/2073153388.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 5, \"encode_fn must have 5 arguments\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/13/2025-19:56:43] [TRT-LLM] [I] Creating session from engine\n",
      "[02/13/2025-19:56:43] [TRT] [I] Loaded engine size: 677 MiB\n",
      "[02/13/2025-19:56:43] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +76, now: CPU 0, GPU 752 (MiB)\n",
      "../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:56:43] [TRT-LLM] [I] Loading engine from ../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:56:43] [TRT-LLM] [I] Creating session from engine\n",
      "[02/13/2025-19:56:43] [TRT] [I] Loaded engine size: 678 MiB\n",
      "[02/13/2025-19:56:43] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +478, now: CPU 0, GPU 1906 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<01:08, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/13/2025-19:56:43] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "[02/13/2025-19:56:44] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:35<00:00, 28.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Time Query Run: 0.003019651412963867\n",
      "Time Passage Run: 0.029099109888076784\n",
      "Time Query Total: 0.0037817022800445558\n",
      "Time Passage Total: 0.030956610918045045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trtllm,\n",
    "    num_passages=10, \n",
    "    qry_model_dir=qry_engine_dir, \n",
    "    ctx_model_dir=ctx_engine_dir, \n",
    "    tokenizer_query=AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base/\"),\n",
    "    tokenizer_ctx=AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base/\"),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")  \n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine calibration (API settings dfefault)\n",
    "\n",
    "STATUS: \n",
    "- [Not supported model](https://github.com/NVIDIA/TensorRT-LLM/issues/1614#issuecomment-2122086630) \n",
    "- Config model type không có Bert - [line 110](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/quantization/quantize_by_modelopt.py#L550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]',\n",
    "                          cache_dir=\"./datahub\")\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]', cache_dir=\"./datahub\")\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calib API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the calibration set and define a forward loop\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "batch_size = 4\n",
    "calib_batches = number_samples*2 // batch_size\n",
    "\n",
    "num_workers = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #! in docker with gpus specify, device_id is 0\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/', return_dict=True)\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/', return_dict=True)\n",
    "\n",
    "query_model.to(device)\n",
    "ctx_model.to(device)\n",
    "print(\"Initialize ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calibrate_loop_query():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_query_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        query_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break\n",
    "\n",
    "def calibrate_loop_ctx():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_ctx_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        ctx_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as atq\n",
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(query_model, config, forward_loop=calibrate_loop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-qry-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=query_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(ctx_model, config, forward_loop=calibrate_loop_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-ctx-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=ctx_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung-10_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
