{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert using docker \n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt-llm version 0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" trtllm_tritonserver bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. comment these following line:\n",
    "    - line 129 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\n",
    "    ```\n",
    "    Comment\n",
    "        if pynvml.__version__ < '11.5.0' or driver_version < '526':\n",
    "            logger.warning(\n",
    "                f'Found pynvml=={pynvml.__version__} and cuda driver version '\n",
    "                f'{driver_version}. Please use pynvml>=11.5.0 and cuda '\n",
    "                f'driver>=526 to get accurate memory usage.')\n",
    "            # Support legacy pynvml. Note that an old API could return\n",
    "            # wrong GPU memory usage.\n",
    "            _device_get_memory_info_fn = pynvml.nvmlDeviceGetMemoryInfo\n",
    "        else:\n",
    "    ```\n",
    "    => Only reserve after `else`, then fix index and save file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### custom mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error:\n",
    "[02/02/2025-04:25:43] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
    "[02/02/2025-04:25:43] [TRT] [E] IBuilder::buildSerializedNetwork: Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: BertRetriever/layers/0/attention/__add___L321/elementwise_binary_L2855/ELEMENTWISE_SUM_0: dimensions not compatible for elementwise. Broadcast has incompatible dimensions: 128 != 256 && 128 != 1 && 256 != 1.)\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/third_party/TensorRT-LLM/examples/bert/build_retrieve.py\", line 273, in <module>\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py\", line 204, in decorated\n",
    "    return f(*args, **kwargs)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 411, in build_engine\n",
    "    assert engine is not None, 'Engine building failed, please check the error log.'\n",
    "AssertionError: Engine building failed, please check the error log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (206555486.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 build_retrieve.py \\\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# add new file for customizing input \n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/bert-retrieve-qry-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32 + bert_attetion_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16 + bert_attetion_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt_llm v0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" heronq02/trtllm-tritonserver:24.12 bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 1 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/home/tiennv/hungnq/rtvserving/mbert-retrieve-qry-base\"   \\\n",
    "    --output_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 1 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/home/tiennv/hungnq/rtvserving/mbert-retrieve-ctx-base\"   \\\n",
    "    --output_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/home/tiennv/hungnq/rtvserving/outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=disable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 8 \\\n",
    "    --max_input_len 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung-10_7/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-13 19:06:46.022789: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739448406.035391 2148610 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739448406.039219 2148610 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 19:06:46.052842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# isort: off\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "# isort: on\n",
    "\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import logger\n",
    "from tensorrt_llm.runtime import Session, TensorInfo\n",
    "\n",
    "# from build import get_engine_name  # isort:skip\n",
    "def get_engine_name(model, dtype, tp_size, rank):\n",
    "    return '{}_{}_tp{}_rank{}.plan'.format(model, dtype, tp_size, rank)\n",
    "\n",
    "def trt_dtype_to_torch(dtype):\n",
    "    if dtype == trt.float16:\n",
    "        return torch.float16\n",
    "    elif dtype == trt.float32:\n",
    "        return torch.float32\n",
    "    elif dtype == trt.int32:\n",
    "        return torch.int32\n",
    "    else:\n",
    "        raise TypeError(\"%s is not supported\" % dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt_llm\n",
    "\n",
    "tensorrt_llm.logger.set_level(\"info\")\n",
    "\n",
    "def get_model_config(config_path):\n",
    "    \n",
    "    config_path = os.path.join(config_path, 'config.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # V0.16.0 add builder \n",
    "    # assert config[\"plugin_config\"][\"remove_input_padding\"] == False, \\\\\n",
    "    # assert config['build_config'][\"plugin_config\"][\"remove_input_padding\"] == False, \\\n",
    "    #     \"Please refer to run_remove_input_padding.py for running BERT models with remove_input_padding enabled\"\n",
    "    return config\n",
    "\n",
    "def get_session(config_path):\n",
    "    config = get_model_config(config_path)\n",
    "    # world_size = config['builder_config']['tensor_parallel']\n",
    "    world_size = config['build_config']['auto_parallel_config']['world_size']\n",
    "    assert world_size == tensorrt_llm.mpi_world_size(), \\\n",
    "        f'Engine world size ({world_size}) != Runtime world size ({tensorrt_llm.mpi_world_size()})'\n",
    "\n",
    "    runtime_rank = tensorrt_llm.mpi_rank() if world_size > 1 else 0\n",
    "    runtime_mapping = tensorrt_llm.Mapping(world_size,\n",
    "                                            runtime_rank,\n",
    "                                            tp_size=world_size)\n",
    "    \n",
    "    torch.cuda.set_device(runtime_rank % runtime_mapping.gpus_per_node)\n",
    "    # dtype = config['builder_config']['precision']\n",
    "    # model_name = config['builder_config']['name']\n",
    "    # serialize_path = get_engine_name(\n",
    "    #     model_name, \n",
    "    #     dtype, world_size, runtime_rank\n",
    "    # )\n",
    "    serialize_path = 'rank0.engine'\n",
    "    serialize_path = os.path.join(config_path, serialize_path)\n",
    "    print(serialize_path)\n",
    "\n",
    "    logger.info(f'Loading engine from {serialize_path}')\n",
    "    with open(serialize_path, 'rb') as f:\n",
    "        engine_buffer = f.read()\n",
    "\n",
    "    logger.info(f'Creating session from engine')\n",
    "    return Session.from_serialized_engine(engine_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def process_input(input_ids_list: List[torch.Tensor],\n",
    "                  token_type_ids_list: List[torch.Tensor],\n",
    "                  is_roberta=False,\n",
    "                  padding_idx=1):\n",
    "    input_lengths = []\n",
    "    position_ids_list = []\n",
    "    max_input_length = 0\n",
    "    for i, input_ids in enumerate(input_ids_list):\n",
    "        input_len = len(input_ids)\n",
    "        assert input_len == len(token_type_ids_list[i]), f\"sample {i}: len(input_ids)={len(input_ids)}, \" \\\n",
    "                                                         f\"len(token_type_ids)={len(token_type_ids_list[i])}, not equal\"\n",
    "        input_lengths.append(input_len)\n",
    "        position_ids = torch.arange(0, input_len, dtype=torch.int32)\n",
    "        if is_roberta:\n",
    "            position_ids = position_ids + 1 + padding_idx\n",
    "\n",
    "        position_ids_list.append(position_ids)\n",
    "        max_input_length = max(max_input_length, input_len)\n",
    "\n",
    "    # [num_tokens]\n",
    "    input_ids = torch.concat(input_ids_list).int().cuda()\n",
    "    token_type_ids = torch.concat(token_type_ids_list).int().cuda()\n",
    "    position_ids = torch.concat(position_ids_list).int().cuda()\n",
    "\n",
    "    input_lengths = torch.tensor(input_lengths).int().cuda()  # [batch_size]\n",
    "    max_input_length = torch.empty((max_input_length, )).int().cuda()\n",
    "    return input_ids, input_lengths, token_type_ids, position_ids, max_input_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def prepare_inputs(texts: list, tokenizer: AutoTokenizer, config) -> dict:\n",
    "    #\n",
    "    remove_padding = config['build_config']['plugin_config'][\n",
    "        'remove_input_padding'\n",
    "    ]\n",
    "    model_name = config['pretrained_config']['architecture']\n",
    "\n",
    "    # Roberta doesn't have token_type_ids, use all zeros to replace\n",
    "    is_roberta = \"Roberta\" in model_name\n",
    "\n",
    "    if remove_padding:\n",
    "        input_ids_list = [\n",
    "            torch.tensor(ids).int().cuda() \\\n",
    "            for ids in inputs_without_padding['input_ids']\n",
    "        ]\n",
    "        # attention_mask_list = inputs_without_padding['attention_mask'],\n",
    "        token_type_ids_list = [\n",
    "            torch.tensor(ids).int().cuda() \\\n",
    "            for ids in inputs_without_padding['token_type_ids']\n",
    "        ]\n",
    "        \n",
    "        input_ids, input_lengths, token_type_ids, position_ids, max_input_length = process_input(\n",
    "            input_ids_list=input_ids_list,\n",
    "            token_type_ids_list=token_type_ids_list,\n",
    "            is_roberta=is_roberta,\n",
    "            padding_idx=config['pretrained_config']['pad_token_id']\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        #NOTE:Padding: pad to longest seq len\n",
    "        inputs_with_padding = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "        inputs_without_padding = tokenizer(texts)\n",
    "\n",
    "        input_ids = torch.tensor(inputs_with_padding['input_ids']).int().cuda()\n",
    "        input_lengths = [len(x) for x in inputs_without_padding['input_ids']]\n",
    "        input_lengths = torch.tensor(input_lengths,\n",
    "                                     device=input_ids.device,\n",
    "                                     dtype=torch.int32)\n",
    "        \n",
    "        # attention_mask = torch.tensor(inputs_with_padding['attention_mask'],\n",
    "        #                               device=input_ids.device,\n",
    "        #                               dtype=torch.int32)\n",
    "        if is_roberta:\n",
    "            token_type_ids = torch.zeros_like(torch.tensor(\n",
    "                inputs_with_padding['input_ids']),\n",
    "                                              device=input_ids.device,\n",
    "                                              dtype=torch.int32)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor(inputs_with_padding['token_type_ids'],\n",
    "                                          device=input_ids.device,\n",
    "                                          dtype=torch.int32)\n",
    "            \n",
    "    # NOTE: TRT-LLM perform inference\n",
    "    if remove_padding:\n",
    "        # NOTE: Remove padding:\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"input_lengths\": input_lengths,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"max_input_length\": max_input_length\n",
    "        }\n",
    "    else:\n",
    "        #NOTE: Padding:\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'input_lengths': input_lengths,\n",
    "            'token_type_ids': token_type_ids,\n",
    "        }\n",
    "    return inputs\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_qry=None, \n",
    "    model_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 5, \"encode_fn must have 5 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        query_config = get_model_config(qry_engine_dir)\n",
    "        stream = torch.cuda.current_stream().cuda_stream\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, config=query_config, stream=stream)\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        passage_config = get_model_config(ctx_engine_dir)\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, config=passage_config, stream=stream)\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run with attention plugin\n",
    "use code run of tensorrtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_engine_dir = \"../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"\n",
    "ctx_engine_dir = \"../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trtllm(texts, model, tokenizer, config, stream):\n",
    "    # check if tokenize length is min 128\n",
    "    inputs_model = prepare_inputs(texts, tokenizer, config)\n",
    "\n",
    "    if config['build_config']['plugin_config']['remove_input_padding']:\n",
    "        output_info = model.infer_shapes([\n",
    "            TensorInfo(\"input_ids\", trt.DataType.INT32, inputs_model['input_ids'].shape),\n",
    "            TensorInfo(\"input_lengths\", trt.DataType.INT32, inputs_model['input_lengths'].shape),\n",
    "            TensorInfo(\"token_type_ids\", trt.DataType.INT32, inputs_model['token_type_ids'].shape),\n",
    "            TensorInfo(\"position_ids\", trt.DataType.INT32, inputs_model['position_ids'].shape),\n",
    "            TensorInfo(\"max_input_length\", trt.DataType.INT32, inputs_model['max_input_length'].shape)\n",
    "        ])\n",
    "    else:\n",
    "        #NOTE: Padding:\n",
    "        output_info = model.infer_shapes([\n",
    "            TensorInfo(\"input_ids\", trt.DataType.INT32, inputs_model['input_ids'].shape),\n",
    "            TensorInfo(\"input_lengths\", trt.DataType.INT32, inputs_model['input_lengths'].shape),\n",
    "            TensorInfo(\"token_type_ids\", trt.DataType.INT32, inputs_model['token_type_ids'].shape),\n",
    "        ])\n",
    "    \n",
    "    print(output_info)\n",
    "    outputs = {\n",
    "        t.name: torch.empty(\n",
    "            tuple(t.shape),\n",
    "            dtype=trt_dtype_to_torch(t.dtype),\n",
    "            device='cuda'\n",
    "        )\n",
    "        for t in output_info\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    ok = model.run(inputs_model, outputs, stream)\n",
    "    assert ok, \"Runtime execution failed\"\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    print(outputs)\n",
    "    # embeddings = outputs['embeddings']\n",
    "    embeddings = outputs['hidden_states']\n",
    "    print(embeddings.shape)\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(len(texts), -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:06:50] [TRT-LLM] [I] Loading engine from ../outputs/trtllm_2/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:06:51] [TRT-LLM] [I] Creating session from engine\n",
      "[02/13/2025-19:06:51] [TRT] [I] Loaded engine size: 677 MiB\n",
      "[02/13/2025-19:06:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +76, now: CPU 0, GPU 752 (MiB)\n",
      "../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:06:51] [TRT-LLM] [I] Loading engine from ../outputs/trtllm_2/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/13/2025-19:06:51] [TRT-LLM] [I] Creating session from engine\n",
      "[02/13/2025-19:06:51] [TRT] [I] Loaded engine size: 678 MiB\n",
      "[02/13/2025-19:06:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +478, now: CPU 0, GPU 1906 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2148610/3233608353.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 5, \"encode_fn must have 5 arguments\"\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorInfo(name='hidden_states', dtype=<DataType.FLOAT: 0>, shape=(1, 7, 768))]\n",
      "[02/13/2025-19:06:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "{'hidden_states': tensor([[[-0.4211, -0.3550,  0.0791,  ...,  0.4089,  0.0313, -0.2890],\n",
      "         [ 0.1998, -0.7420, -0.0119,  ...,  0.4589, -0.9639,  0.5904],\n",
      "         [-0.4101,  0.2290,  0.0470,  ..., -0.3395,  0.7337, -1.1101],\n",
      "         ...,\n",
      "         [-0.4625, -0.0845, -0.2683,  ...,  0.2754, -0.7698, -0.1823],\n",
      "         [ 0.1476,  1.4050,  0.4777,  ...,  0.3627, -0.2865,  0.0105],\n",
      "         [-0.1123, -0.3258,  0.1678,  ...,  0.2707, -0.1695, -0.7328]]],\n",
      "       device='cuda:0')}\n",
      "torch.Size([1, 7, 768])\n",
      "[TensorInfo(name='hidden_states', dtype=<DataType.FLOAT: 0>, shape=(10, 179, 768))]\n",
      "[02/13/2025-19:06:52] [TRT] [W] Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "{'hidden_states': tensor([[[-0.3246, -0.3754, -0.0927,  ...,  0.1979,  0.0347, -0.2672],\n",
      "         [ 0.2720, -0.9532,  0.0034,  ...,  0.3344, -1.0428,  0.3251],\n",
      "         [-0.5846,  0.3533, -0.0236,  ..., -0.6789,  0.5855, -1.0793],\n",
      "         ...,\n",
      "         [-0.0728, -0.3804, -0.1995,  ...,  0.9071,  0.5576, -0.5274],\n",
      "         [-0.5991, -0.2016, -0.1465,  ...,  0.5983, -0.1494,  0.2584],\n",
      "         [-0.3585, -1.4983, -0.1424,  ...,  1.0038, -0.6010,  1.5604]],\n",
      "\n",
      "        [[ 0.3587,  0.0503, -0.0374,  ...,  0.4469,  0.8176, -0.3787],\n",
      "         [-0.0435, -1.5507, -0.7408,  ...,  0.4613, -0.2097, -0.3125],\n",
      "         [ 0.1821,  0.4282, -0.7348,  ..., -0.3960, -1.4592, -0.7314],\n",
      "         ...,\n",
      "         [-0.0403, -0.2847,  0.1928,  ...,  0.4226, -1.2336,  0.1656],\n",
      "         [ 0.5041, -0.5845,  0.2562,  ...,  0.3512, -0.3197,  0.3008],\n",
      "         [-0.0635, -0.5968,  0.3527,  ...,  0.0772,  0.6440,  0.1902]],\n",
      "\n",
      "        [[-0.6073, -1.7067,  0.2855,  ...,  0.7916, -0.0621,  1.2800],\n",
      "         [-0.2599, -0.4735,  0.0887,  ..., -0.1087,  1.8186, -0.8730],\n",
      "         [-0.3522, -0.7253,  0.1419,  ...,  0.9824, -0.1429,  0.1183],\n",
      "         ...,\n",
      "         [-0.4985,  0.3410, -0.9496,  ...,  0.7169,  0.5413, -0.0557],\n",
      "         [-0.2400,  0.5041, -0.5179,  ...,  0.6470, -0.4520,  0.1905],\n",
      "         [ 0.2738, -0.1449,  1.3763,  ...,  1.1617, -0.1049,  0.2368]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3089, -0.7508,  0.1051,  ...,  0.5749,  0.1392, -0.6366],\n",
      "         [-0.7183, -0.0496, -0.6397,  ...,  0.5337,  0.2195,  0.0693],\n",
      "         [ 0.6920, -0.7953,  0.3530,  ...,  0.5220, -0.6717,  0.1946],\n",
      "         ...,\n",
      "         [ 0.0144, -0.7828, -0.2207,  ..., -0.3459, -0.5389,  0.7274],\n",
      "         [ 0.5953, -0.0474,  0.3422,  ...,  1.3584,  0.2335, -1.3510],\n",
      "         [-0.0550, -0.8401, -0.2032,  ..., -0.0971, -0.1403, -0.5435]],\n",
      "\n",
      "        [[-0.4323, -0.3058, -0.6140,  ...,  0.3775, -0.4778,  0.2126],\n",
      "         [ 0.6448,  0.4369,  0.0472,  ...,  0.8021,  0.3295, -0.6500],\n",
      "         [-0.4978, -0.3180,  0.0397,  ..., -0.1270,  0.1515,  0.7647],\n",
      "         ...,\n",
      "         [ 0.5491, -0.9267, -0.3121,  ...,  0.1934, -0.3838, -0.3622],\n",
      "         [-0.3769, -1.4937, -0.4733,  ...,  0.0309,  0.0758,  0.7038],\n",
      "         [-1.2967, -0.5877, -0.7513,  ...,  0.0503,  0.0181, -0.5075]],\n",
      "\n",
      "        [[-0.8160, -0.0974, -0.7170,  ...,  0.8080, -0.1765,  0.6473],\n",
      "         [-0.5103, -0.0237, -0.5051,  ...,  0.4046, -1.0346, -0.5692],\n",
      "         [ 0.6121, -0.0752, -0.6657,  ...,  0.4435,  0.1825, -0.8693],\n",
      "         ...,\n",
      "         [-0.4743,  0.2853,  1.0671,  ...,  0.9411,  0.2832,  0.0409],\n",
      "         [ 0.6022,  0.2462, -0.4805,  ...,  0.2628, -0.4905, -0.6174],\n",
      "         [-0.6673, -0.2595, -0.6236,  ..., -0.2608,  0.1807,  1.6315]]],\n",
      "       device='cuda:0')}\n",
      "torch.Size([10, 179, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 2\u001b[0m accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total \u001b[38;5;241m=\u001b[39m \u001b[43meval_accuracy_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_trtllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_passages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqry_engine_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_engine_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../mbert-retrieve-qry-base/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../mbert-retrieve-ctx-base/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Query Run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_query_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[7], line 64\u001b[0m, in \u001b[0;36meval_accuracy_trt\u001b[0;34m(data, encode_fn, num_passages, model_qry, model_ctx, tokenizer_query, tokenizer_ctx, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# accuracy\u001b[39;00m\n\u001b[1;32m     63\u001b[0m scores \u001b[38;5;241m=\u001b[39m compute_similarity(query, encoded_passages)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     66\u001b[0m accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trtllm,\n",
    "    num_passages=10, \n",
    "    model_qry=get_session(qry_engine_dir), \n",
    "    model_ctx=get_session(ctx_engine_dir), \n",
    "    tokenizer_query=AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base/\"),\n",
    "    tokenizer_ctx=AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base/\"),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")  \n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine calibration (API settings dfefault)\n",
    "\n",
    "STATUS: \n",
    "- [Not supported model](https://github.com/NVIDIA/TensorRT-LLM/issues/1614#issuecomment-2122086630) \n",
    "- Config model type không có Bert - [line 110](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/quantization/quantize_by_modelopt.py#L550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]',\n",
    "                          cache_dir=\"./datahub\")\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]', cache_dir=\"./datahub\")\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calib API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the calibration set and define a forward loop\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "batch_size = 4\n",
    "calib_batches = number_samples*2 // batch_size\n",
    "\n",
    "num_workers = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #! in docker with gpus specify, device_id is 0\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/', return_dict=True)\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/', return_dict=True)\n",
    "\n",
    "query_model.to(device)\n",
    "ctx_model.to(device)\n",
    "print(\"Initialize ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calibrate_loop_query():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_query_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        query_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break\n",
    "\n",
    "def calibrate_loop_ctx():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_ctx_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        ctx_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as atq\n",
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(query_model, config, forward_loop=calibrate_loop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-qry-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=query_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(ctx_model, config, forward_loop=calibrate_loop_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-ctx-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=ctx_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung-10_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
