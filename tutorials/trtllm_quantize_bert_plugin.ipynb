{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert using docker \n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt-llm version 0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" trtllm_tritonserver bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. comment these following line:\n",
    "    - line 129 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\n",
    "    ```\n",
    "    Comment\n",
    "        if pynvml.__version__ < '11.5.0' or driver_version < '526':\n",
    "            logger.warning(\n",
    "                f'Found pynvml=={pynvml.__version__} and cuda driver version '\n",
    "                f'{driver_version}. Please use pynvml>=11.5.0 and cuda '\n",
    "                f'driver>=526 to get accurate memory usage.')\n",
    "            # Support legacy pynvml. Note that an old API could return\n",
    "            # wrong GPU memory usage.\n",
    "            _device_get_memory_info_fn = pynvml.nvmlDeviceGetMemoryInfo\n",
    "        else:\n",
    "    ```\n",
    "    => Only reserve after `else`, then fix index and save file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### custom mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error:\n",
    "[02/02/2025-04:25:43] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
    "[02/02/2025-04:25:43] [TRT] [E] IBuilder::buildSerializedNetwork: Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: BertRetriever/layers/0/attention/__add___L321/elementwise_binary_L2855/ELEMENTWISE_SUM_0: dimensions not compatible for elementwise. Broadcast has incompatible dimensions: 128 != 256 && 128 != 1 && 256 != 1.)\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/third_party/TensorRT-LLM/examples/bert/build_retrieve.py\", line 273, in <module>\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py\", line 204, in decorated\n",
    "    return f(*args, **kwargs)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 411, in build_engine\n",
    "    assert engine is not None, 'Engine building failed, please check the error log.'\n",
    "AssertionError: Engine building failed, please check the error log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (206555486.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 build_retrieve.py \\\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# add new file for customizing input \n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/bert-retrieve-qry-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32 + bert_attetion_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16 + bert_attetion_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorrt_llm v0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" trtllm_tritonserver bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 8 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --dtype float32  \\\n",
    "    --tp_size 1\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float32 \\\n",
    "    --max_batch_size 16 \\\n",
    "    --max_input_len 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.16\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-qry-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 10 \\\n",
    "    --max_input_len 512\n",
    "\n",
    "python3 convert_checkpoint.py \\\n",
    "    --model BertModel  \\\n",
    "    --model_dir \"/data/mbert-retrieve-ctx-base/\"   \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --dtype float16  \\\n",
    "    --tp_size 2\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\"  \\\n",
    "    --output_dir=\"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint_2\" \\\n",
    "    --remove_input_padding=enable \\\n",
    "    --bert_attention_plugin=float16 \\\n",
    "    --max_batch_size 8 \\\n",
    "    --max_input_len 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung-10_7/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# becnhmark run onnx model\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        # self.inputs, self.outputs, self.bindings = self.allocate_buffers()\n",
    "        self.stream = cuda.Stream()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self, binding_shape, input_lengths):\n",
    "        # Allocate host and device buffers\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        for binding in self.engine:\n",
    "            \n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT and binding != \"input_lengths\":\n",
    "                self.context.set_input_shape(binding, binding_shape)\n",
    "            elif binding == \"input_lengths\": \n",
    "                self.context.set_input_shape(binding, input_lengths)\n",
    "\n",
    "            # print(\"binding: \", binding)\n",
    "            size = trt.volume(self.context.get_tensor_shape(binding))\n",
    "            # print(\"size: \", size)\n",
    "            # print(\"batch_size: \", self.context.get_tensor_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_tensor_dtype(binding))\n",
    "            # print(\"dtype: \", dtype)\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            # if self.engine.binding_is_input(binding):\n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "\n",
    "        return inputs, outputs, bindings\n",
    "        \n",
    "            \n",
    "    def __call__(self, inputs_id, attention_mask, token_type_ids, batch_size=2):\n",
    "\n",
    "        \n",
    "        x = np.array(inputs_id).astype(self.dtype)\n",
    "        y = np.array(attention_mask).astype(self.dtype)\n",
    "        z = np.array(token_type_ids).astype(self.dtype)\n",
    "        input_lengths = np.array([batch_size]).astype(self.dtype)\n",
    "\n",
    "        inputs, outputs, bindings = self.allocate_buffers(x.shape, input_lengths)\n",
    "    \n",
    "            \n",
    "        # Transfer input data to the GPU.\n",
    "        # print(x.shape)\n",
    "        np.copyto(inputs[0].host,x.ravel())\n",
    "        np.copyto(inputs[1].host,y.ravel())\n",
    "        np.copyto(inputs[2].host,z.ravel())\n",
    "        np.copyto(inputs[3].host, input_lengths.ravel())\n",
    "        \n",
    "        # after copy -> transfer to device, transer first will error duo to hold old value\n",
    "        for inp in inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "\n",
    "        # Run inference\n",
    "        self.context.execute_v2(bindings=bindings)\n",
    "        \n",
    "        # Transfer prediction output from the GPU.\n",
    "        for out in outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        \n",
    "        # Synchronize the stream\n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(batch_size,-1) for out in outputs]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trt(texts, model, tokenizer, batch_size):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model(\n",
    "        encoded_input['input_ids'],\n",
    "        encoded_input['attention_mask'],\n",
    "        encoded_input['token_type_ids'],\n",
    "        batch_size\n",
    "    )[0]\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_qry=None, \n",
    "    model_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, len(query_batch))\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, len(concate_passage))\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/12/2025-10:15:15] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 1: Internal Error (Failed due to an old deserialization call on a newer plan file. This might happen when the plan file was built from an older TensorRT version. You can use `trtexec --getPlanVersionOnly` to check the version of TensorRT that was used to create the plan file.)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create_execution_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m trt_engine_qry_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.plan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m trt_engine_ctx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.plan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m model_query \u001b[38;5;241m=\u001b[39m \u001b[43mTrtModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrt_engine_qry_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model_ctx \u001b[38;5;241m=\u001b[39m TrtModel(trt_engine_ctx_path, max_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     12\u001b[0m tokenizer_qry \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../mbert-retrieve-qry-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mTrtModel.__init__\u001b[0;34m(self, engine_path, max_batch_size, dtype)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# self.inputs, self.outputs, self.bindings = self.allocate_buffers()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mStream()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_execution_context\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_execution_context'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# trtversion 10.3.0 - trtllm 0.12.0.dev...\n",
    "# trt_engine_qry_path = \"../outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.plan\"\n",
    "# trt_engine_ctx_path = \"../outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.plan\"\n",
    "\n",
    "trt_engine_qry_path = \"../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.plan\"\n",
    "trt_engine_ctx_path = \"../outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.plan\"\n",
    "\n",
    "model_query = TrtModel(trt_engine_qry_path, max_batch_size=1, dtype=np.int32)\n",
    "model_ctx = TrtModel(trt_engine_ctx_path, max_batch_size=10, dtype=np.int32)\n",
    "tokenizer_qry = AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1457919/162317057.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total \u001b[38;5;241m=\u001b[39m \u001b[43meval_accuracy_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_trt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_passages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_qry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Query Run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_query_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36meval_accuracy_trt\u001b[0;34m(data, encode_fn, num_passages, model_qry, model_ctx, tokenizer_query, tokenizer_ctx, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#! CHANGE HERE\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m encoded_passages, time_ctx \u001b[38;5;241m=\u001b[39m \u001b[43mencode_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcate_passage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconcate_passage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     56\u001b[0m time_passage_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end_time\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mencode_trt\u001b[0;34m(texts, model, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      6\u001b[0m     texts, \n\u001b[1;32m      7\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(embeddings.reshape(batch_size, -1, 768))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mTrtModel.__call__\u001b[0;34m(self, inputs_id, attention_mask, token_type_ids, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(token_type_ids)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     82\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([batch_size])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 84\u001b[0m inputs, outputs, bindings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocate_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Transfer input data to the GPU.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m np\u001b[38;5;241m.\u001b[39mcopyto(inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhost,x\u001b[38;5;241m.\u001b[39mravel())\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mTrtModel.allocate_buffers\u001b[0;34m(self, binding_shape, input_lengths)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m binding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mget_tensor_mode(binding) \u001b[38;5;241m==\u001b[39m trt\u001b[38;5;241m.\u001b[39mTensorIOMode\u001b[38;5;241m.\u001b[39mINPUT \u001b[38;5;129;01mand\u001b[39;00m binding \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_input_shape\u001b[49m(binding, binding_shape)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m binding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mset_input_shape(binding, input_lengths)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_input_shape'"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trt,\n",
    "    num_passages=10, \n",
    "    model_ctx=model_ctx,\n",
    "    model_qry=model_query, \n",
    "    tokenizer_ctx=tokenizer_ctx,\n",
    "    tokenizer_query=tokenizer_qry,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run with attention plugin\n",
    "use code run of tensorrtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 10:57:04.482372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739332624.495831 1784633 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739332624.499844 1784633 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-12 10:57:04.514305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# isort: off\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "# isort: on\n",
    "\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import logger\n",
    "from tensorrt_llm.runtime import Session, TensorInfo\n",
    "\n",
    "# from build import get_engine_name  # isort:skip\n",
    "def get_engine_name(model, dtype, tp_size, rank):\n",
    "    return '{}_{}_tp{}_rank{}.plan'.format(model, dtype, tp_size, rank)\n",
    "\n",
    "def trt_dtype_to_torch(dtype):\n",
    "    if dtype == trt.float16:\n",
    "        return torch.float16\n",
    "    elif dtype == trt.float32:\n",
    "        return torch.float32\n",
    "    elif dtype == trt.int32:\n",
    "        return torch.int32\n",
    "    else:\n",
    "        raise TypeError(\"%s is not supported\" % dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_engine_dir = \"../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/\"\n",
    "ctx_engine_dir = \"../outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint_2/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt_llm\n",
    "\n",
    "tensorrt_llm.logger.set_level(\"info\")\n",
    "\n",
    "def get_model_config(config_path):\n",
    "    \n",
    "    config_path = os.path.join(qry_engine_dir, 'config.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # V0.16.0 add builder \n",
    "    # assert config[\"plugin_config\"][\"remove_input_padding\"] == False, \\\\\n",
    "    # assert config['build_config'][\"plugin_config\"][\"remove_input_padding\"] == False, \\\n",
    "    #     \"Please refer to run_remove_input_padding.py for running BERT models with remove_input_padding enabled\"\n",
    "    return config\n",
    "\n",
    "def get_session(config_path):\n",
    "    config = get_model_config(config_path)\n",
    "    # world_size = config['builder_config']['tensor_parallel']\n",
    "    world_size = config['build_config']['auto_parallel_config']['world_size']\n",
    "    assert world_size == tensorrt_llm.mpi_world_size(), \\\n",
    "        f'Engine world size ({world_size}) != Runtime world size ({tensorrt_llm.mpi_world_size()})'\n",
    "\n",
    "    runtime_rank = tensorrt_llm.mpi_rank() if world_size > 1 else 0\n",
    "    runtime_mapping = tensorrt_llm.Mapping(world_size,\n",
    "                                            runtime_rank,\n",
    "                                            tp_size=world_size)\n",
    "    \n",
    "    torch.cuda.set_device(runtime_rank % runtime_mapping.gpus_per_node)\n",
    "    # dtype = config['builder_config']['precision']\n",
    "    # model_name = config['builder_config']['name']\n",
    "    # serialize_path = get_engine_name(\n",
    "    #     model_name, \n",
    "    #     dtype, world_size, runtime_rank\n",
    "    # )\n",
    "    serialize_path = 'rank0.engine'\n",
    "    serialize_path = os.path.join(qry_engine_dir, serialize_path)\n",
    "    print(serialize_path)\n",
    "\n",
    "    logger.info(f'Loading engine from {serialize_path}')\n",
    "    with open(serialize_path, 'rb') as f:\n",
    "        engine_buffer = f.read()\n",
    "\n",
    "    logger.info(f'Creating session from engine')\n",
    "    return Session.from_serialized_engine(engine_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trtllm(texts, model, tokenizer, stream):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt', # need return pt here\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    output_info = model.infer_shapes([\n",
    "        TensorInfo('input_ids', trt.DataType.INT32, encoded_input['input_ids'].numpy().ravel().shape), # flatten to set shape\n",
    "        # TensorInfo('attention_mask', trt.DataType.INT32, encoded_input['attention_mask'].shape),\n",
    "        TensorInfo('token_type_ids', trt.DataType.INT32, encoded_input['token_type_ids'].numpy().ravel().shape),\n",
    "    ])\n",
    "    print(output_info)\n",
    "    outputs = {\n",
    "        t.name: torch.empty(\n",
    "            tuple((len(texts), t.shape[-1])),\n",
    "            dtype=trt_dtype_to_torch(t.dtype),\n",
    "            device='cuda'\n",
    "        )\n",
    "        for t in output_info\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    ok = model.run(encoded_input, outputs, stream)\n",
    "    assert ok, \"Runtime execution failed\"\n",
    "        \n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    embeddings = outputs['embeddings']\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(len(texts), -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/12/2025-11:00:41] [TRT-LLM] [I] Loading engine from ../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/12/2025-11:00:41] [TRT-LLM] [I] Creating session from engine\n",
      "[02/12/2025-11:00:41] [TRT] [I] Loaded engine size: 677 MiB\n",
      "[02/12/2025-11:00:41] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[02/12/2025-11:00:41] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +77, now: CPU 0, GPU 2258 (MiB)\n",
      "../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/12/2025-11:00:41] [TRT-LLM] [I] Loading engine from ../outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint_2/rank0.engine\n",
      "[02/12/2025-11:00:41] [TRT-LLM] [I] Creating session from engine\n",
      "[02/12/2025-11:00:41] [TRT] [I] Loaded engine size: 677 MiB\n",
      "[02/12/2025-11:00:41] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1784633/162317057.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/12/2025-11:00:42] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +76, now: CPU 0, GPU 3010 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorInfo(name='hidden_states', dtype=<DataType.FLOAT: 0>, shape=(-1, 768))]\n",
      "[02/12/2025-11:00:42] [TRT] [E] IExecutionContext::enqueueV3: Error Code 3: API Usage Error (Parameter check failed, condition: mContext.profileObliviousBindings.at(profileObliviousIndex) != nullptr. Address is not set for input tensor position_ids. Call setInputTensorAddress or setTensorAddress before enqueue/execute.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Runtime execution failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total \u001b[38;5;241m=\u001b[39m \u001b[43meval_accuracy_trt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_trtllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_passages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx_engine_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqry_engine_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../mbert-retrieve-ctx-base/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../mbert-retrieve-qry-base/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Query Run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_query_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36meval_accuracy_trt\u001b[0;34m(data, encode_fn, num_passages, model_qry, model_ctx, tokenizer_query, tokenizer_ctx, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#! CHANGE HERE\u001b[39;00m\n\u001b[1;32m     44\u001b[0m query_batch \u001b[38;5;241m=\u001b[39m [data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 45\u001b[0m query, time_query \u001b[38;5;241m=\u001b[39m \u001b[43mencode_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_qry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquery_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     47\u001b[0m time_query_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end_time\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mencode_trtllm\u001b[0;34m(texts, model, tokenizer, stream)\u001b[0m\n\u001b[1;32m     32\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m ok \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun(encoded_input, outputs, stream)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ok, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime execution failed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Runtime execution failed"
     ]
    }
   ],
   "source": [
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trtllm,\n",
    "    num_passages=10, \n",
    "    model_ctx=get_session(ctx_engine_dir), \n",
    "    model_qry=get_session(qry_engine_dir), \n",
    "    tokenizer_ctx=AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base/\"),\n",
    "    tokenizer_query=AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base/\"),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")  \n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine calibration (API settings dfefault)\n",
    "\n",
    "STATUS: \n",
    "- [Not supported model](https://github.com/NVIDIA/TensorRT-LLM/issues/1614#issuecomment-2122086630) \n",
    "- Config model type không có Bert - [line 110](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/quantization/quantize_by_modelopt.py#L550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]',\n",
    "                          cache_dir=\"./datahub\")\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]', cache_dir=\"./datahub\")\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calib API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the calibration set and define a forward loop\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "batch_size = 4\n",
    "calib_batches = number_samples*2 // batch_size\n",
    "\n",
    "num_workers = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #! in docker with gpus specify, device_id is 0\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/', return_dict=True)\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/', return_dict=True)\n",
    "\n",
    "query_model.to(device)\n",
    "ctx_model.to(device)\n",
    "print(\"Initialize ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calibrate_loop_query():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_query_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        query_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break\n",
    "\n",
    "def calibrate_loop_ctx():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_ctx_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        ctx_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as atq\n",
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(query_model, config, forward_loop=calibrate_loop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-qry-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=query_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(ctx_model, config, forward_loop=calibrate_loop_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-ctx-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=ctx_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung-10_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
