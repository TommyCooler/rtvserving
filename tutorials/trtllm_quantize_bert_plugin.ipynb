{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert using docker \n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dung voi tensorrt-llm version 0.12.0\n",
    "1. docker run -it --rm --network host -v ./:/data --gpus \"device=1\" trtllm_tritonserver bash\n",
    "2. pip install pycuda tensorflow h5py==3.10.0 'transformers<=4.42.4'\n",
    "3. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "4. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. comment these following line:\n",
    "    - line 129 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\n",
    "    ```\n",
    "    Comment\n",
    "        if pynvml.__version__ < '11.5.0' or driver_version < '526':\n",
    "            logger.warning(\n",
    "                f'Found pynvml=={pynvml.__version__} and cuda driver version '\n",
    "                f'{driver_version}. Please use pynvml>=11.5.0 and cuda '\n",
    "                f'driver>=526 to get accurate memory usage.')\n",
    "            # Support legacy pynvml. Note that an old API could return\n",
    "            # wrong GPU memory usage.\n",
    "            _device_get_memory_info_fn = pynvml.nvmlDeviceGetMemoryInfo\n",
    "        else:\n",
    "    ```\n",
    "    => Only reserve after `else`, then fix index and save file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py     --model BertModel     --model_dir=\"/data/mbert-retrieve-ctx-base/\"     --dtype=float32 --log_level=verbose     --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano /usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/bert/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano /usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error:\n",
    "[02/02/2025-04:25:43] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
    "[02/02/2025-04:25:43] [TRT] [E] IBuilder::buildSerializedNetwork: Error Code 4: Internal Error (kOPT values for profile 0 violate shape constraints: BertRetriever/layers/0/attention/__add___L321/elementwise_binary_L2855/ELEMENTWISE_SUM_0: dimensions not compatible for elementwise. Broadcast has incompatible dimensions: 128 != 256 && 128 != 1 && 256 != 1.)\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/third_party/TensorRT-LLM/examples/bert/build_retrieve.py\", line 273, in <module>\n",
    "    engine = builder.build_engine(network, builder_config)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py\", line 204, in decorated\n",
    "    return f(*args, **kwargs)\n",
    "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 411, in build_engine\n",
    "    assert engine is not None, 'Engine building failed, please check the error log.'\n",
    "AssertionError: Engine building failed, please check the error log.\n",
    "\n",
    "=> do mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/bert-retrieve-qry-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_tllm_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp32 + bert_attetion_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp16 + bert_attetion_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build_retrieve.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/outputs/trtllm/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# becnhmark run onnx model\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        # self.inputs, self.outputs, self.bindings = self.allocate_buffers()\n",
    "        self.stream = cuda.Stream()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self, binding_shape):\n",
    "        # Allocate host and device buffers\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        for binding in self.engine:\n",
    "            # \n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                self.context.set_input_shape(binding, binding_shape)\n",
    "                \n",
    "            print(\"binding: \", binding)\n",
    "            size = trt.volume(self.context.get_tensor_shape(binding))\n",
    "            print(\"size: \", size)\n",
    "            print(\"batch_size: \", self.context.get_tensor_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_tensor_dtype(binding))\n",
    "            print(\"dtype: \", dtype)\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            # if self.engine.binding_is_input(binding):\n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "\n",
    "        return inputs, outputs, bindings\n",
    "        \n",
    "            \n",
    "    def __call__(self, inputs_id, attention_mask, token_type_ids, batch_size=2):\n",
    "\n",
    "        \n",
    "        x = np.array(inputs_id).astype(self.dtype)\n",
    "        y = np.array(attention_mask).astype(self.dtype)\n",
    "        z = np.array(token_type_ids).astype(self.dtype)\n",
    "\n",
    "\n",
    "        inputs, outputs, bindings = self.allocate_buffers(x.shape)\n",
    "    \n",
    "            \n",
    "        # Transfer input data to the GPU.\n",
    "        # print(x.shape)\n",
    "        np.copyto(inputs[0].host,x.ravel())\n",
    "        np.copyto(inputs[1].host,y.ravel())\n",
    "        np.copyto(inputs[2].host,z.ravel())\n",
    "        \n",
    "        # after copy -> transfer to device, transer first will error duo to hold old value\n",
    "        for inp in inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "\n",
    "        # Run inference\n",
    "        self.context.execute_async_v2(bindings=bindings, stream_handle=self.stream.handle)\n",
    "        \n",
    "        # Transfer prediction output from the GPU.\n",
    "        for out in outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        \n",
    "        # Synchronize the stream\n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(batch_size,-1) for out in outputs]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trt(texts, model, tokenizer, batch_size):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model(\n",
    "        encoded_input['input_ids'],\n",
    "        encoded_input['attention_mask'],\n",
    "        encoded_input['token_type_ids'],\n",
    "        batch_size\n",
    "    )[0]\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy_trt(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_qry=None, \n",
    "    model_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    tokenizer_ctx=None, \n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        query_batch = [data[i]['query']]\n",
    "        query, time_query = encode_fn(query_batch, model_qry, tokenizer_query, len(query_batch))\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        #! CHANGE HERE\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, len(concate_passage))\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at ../datahub/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Sun Jan 26 14:00:08 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"../datahub/\")\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/02/2025-14:30:52] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 1: Internal Error (Failed due to an old deserialization call on a newer plan file. This might happen when the plan file was built from an older TensorRT version. You can use `trtexec --getPlanVersionOnly` to check the version of TensorRT that was used to create the plan file.)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'create_execution_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m trt_engine_ctx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin/BertModel_float32_tp1_rank0.plan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# trt_engine_qry_path = \"./mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m model_query \u001b[38;5;241m=\u001b[39m \u001b[43mTrtModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrt_engine_qry_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m model_ctx \u001b[38;5;241m=\u001b[39m TrtModel(trt_engine_ctx_path, max_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     26\u001b[0m tokenizer_qry \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx_convert_outputs/mbert-retrieve-qry-onnx/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m, in \u001b[0;36mTrtModel.__init__\u001b[0;34m(self, engine_path, max_batch_size, dtype)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# self.inputs, self.outputs, self.bindings = self.allocate_buffers()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mStream()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_execution_context\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'create_execution_context'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# trt_version 8.6.1\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "# trtversion 10.3.0\n",
    "\n",
    "# trt_engine_qry_path = \"../outputs/trtllm/mbert-retrieve-qry-base_float32_bert_atten_plugin/BertModel_float32_tp1_rank0.plan\"\n",
    "# trt_engine_ctx_path = \"../outputs/trtllm/mbert-retrieve-ctx-base_float32_bert_atten_plugin/BertModel_float32_tp1_rank0.plan\"\n",
    "\n",
    "# trt_engine_qry_path = \"./mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "# trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "\n",
    "model_query = TrtModel(trt_engine_qry_path, max_batch_size=1, dtype=np.int32)\n",
    "model_ctx = TrtModel(trt_engine_ctx_path, max_batch_size=10, dtype=np.int32)\n",
    "tokenizer_qry = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-qry-onnx/\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-ctx-onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trt,\n",
    "    num_passages=10, \n",
    "    model_ctx=model_ctx,\n",
    "    model_qry=model_query, \n",
    "    tokenizer_ctx=tokenizer_ctx,\n",
    "    tokenizer_query=tokenizer_qry,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run with attention plugin\n",
    "use code run of tensorrtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# trt llm run.py\n",
    "\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# isort: off\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "# isort: on\n",
    "\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import logger\n",
    "from tensorrt_llm.runtime import Session, TensorInfo\n",
    "\n",
    "# from build import get_engine_name  # isort:skip\n",
    "def get_engine_name(model, dtype, tp_size, rank):\n",
    "    return '{}_{}_tp{}_rank{}.engine'.format(model, dtype, tp_size, rank)\n",
    "\n",
    "def trt_dtype_to_torch(dtype):\n",
    "    if dtype == trt.float16:\n",
    "        return torch.float16\n",
    "    elif dtype == trt.float32:\n",
    "        return torch.float32\n",
    "    elif dtype == trt.int32:\n",
    "        return torch.int32\n",
    "    else:\n",
    "        raise TypeError(\"%s is not supported\" % dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_engine_dir = \"\"\n",
    "ctx_engine_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt_llm\n",
    "\n",
    "tensorrt_llm.logger.set_level(\"info\")\n",
    "\n",
    "def get_model_config(config_path):\n",
    "    \n",
    "    config_path = os.path.join(qry_engine_dir, 'config.json')\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    assert config[\"plugin_config\"][\"remove_input_padding\"] == False, \\\n",
    "        \"Please refer to run_remove_input_padding.py for running BERT models with remove_input_padding enabled\"\n",
    "    return config\n",
    "\n",
    "def get_session(config_path):\n",
    "    config = get_model_config(config_path)\n",
    "    world_size = config['builder_config']['tensor_parallel']\n",
    "    assert world_size == tensorrt_llm.mpi_world_size(), \\\n",
    "        f'Engine world size ({world_size}) != Runtime world size ({tensorrt_llm.mpi_world_size()})'\n",
    "\n",
    "    runtime_rank = tensorrt_llm.mpi_rank() if world_size > 1 else 0\n",
    "    runtime_mapping = tensorrt_llm.Mapping(world_size,\n",
    "                                            runtime_rank,\n",
    "                                            tp_size=world_size)\n",
    "    \n",
    "    torch.cuda.set_device(runtime_rank % runtime_mapping.gpus_per_node)\n",
    "    dtype = config['builder_config']['precision']\n",
    "    model_name = config['builder_config']['name']\n",
    "    serialize_path = get_engine_name(\n",
    "        model_name, \n",
    "        dtype, world_size, runtime_rank\n",
    "    )\n",
    "    serialize_path = os.path.join(qry_engine_dir, serialize_path)\n",
    "\n",
    "    logger.info(f'Loading engine from {serialize_path}')\n",
    "    with open(serialize_path, 'rb') as f:\n",
    "        engine_buffer = f.read()\n",
    "\n",
    "    logger.info(f'Creating session from engine')\n",
    "    return Session.from_serialized_engine(engine_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trtllm(texts, model, tokenizer, batch_size, stream):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    output_info = model.infer_shapes([\n",
    "        TensorInfo('input_ids', trt.DataType.INT32, encoded_input['input_ids'].shape),\n",
    "        TensorInfo('attention_mask', trt.DataType.INT32, encoded_input['attention_mask'].shape),\n",
    "        TensorInfo('token_type_ids', trt.DataType.INT32, encoded_input['token_type_ids'].shape),\n",
    "    ])\n",
    "    outputs = {\n",
    "        t.name: torch.empty(\n",
    "            tuple(t.shape),\n",
    "            dtype=trt_dtype_to_torch(t.dtype),\n",
    "            device='cuda'\n",
    "        )\n",
    "        for t in output_info\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    ok = model.run(encoded_input, outputs, stream)\n",
    "    assert ok, \"Runtime execution failed\"\n",
    "        \n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    embeddings = outputs['embeddings']\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trtllm,\n",
    "    num_passages=10, \n",
    "    model_ctx=get_session(ctx_engine_dir), \n",
    "    model_qry=get_session(qry_engine_dir), \n",
    "    tokenizer_ctx=AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-ctx-onnx/\"),\n",
    "    tokenizer_query=AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-qry-onnx/\"),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")  \n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine calibration (API settings dfefault)\n",
    "\n",
    "STATUS: \n",
    "- [Not supported model](https://github.com/NVIDIA/TensorRT-LLM/issues/1614#issuecomment-2122086630) \n",
    "- Config model type không có Bert - [line 110](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/quantization/quantize_by_modelopt.py#L550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]',\n",
    "                          cache_dir=\"./datahub\")\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]', cache_dir=\"./datahub\")\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calib API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the calibration set and define a forward loop\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "batch_size = 4\n",
    "calib_batches = number_samples*2 // batch_size\n",
    "\n",
    "num_workers = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #! in docker with gpus specify, device_id is 0\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/', return_dict=True)\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/', return_dict=True)\n",
    "\n",
    "query_model.to(device)\n",
    "ctx_model.to(device)\n",
    "print(\"Initialize ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calibrate_loop_query():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_query_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        query_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break\n",
    "\n",
    "def calibrate_loop_ctx():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_ctx_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        ctx_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as atq\n",
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(query_model, config, forward_loop=calibrate_loop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-qry-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=query_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(ctx_model, config, forward_loop=calibrate_loop_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-ctx-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=ctx_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
