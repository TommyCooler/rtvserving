{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert using docker \n",
    "https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dung voi tensorrt-llm version 0.12.0\n",
    "1. docker run -it --rm --network host -v ./:/opt/tritonserver/data --gpus \"device=1\" heronq/trtllmserver:latest bash\n",
    "2. cd /data/third_party/TensorRT-LLM/examples/bert\n",
    "3. code convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-qry-base_float32_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-ctx-base_float32_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-qry-base_float16_tllm_checkpoint\"\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float16 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-ctx-base_float16_tllm_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp32 + bert_attetion_plugin float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-qry-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-ctx-base_float32_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp16 + bert_attetion_plugin float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-qry-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-qry-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16\n",
    "\n",
    "python3 build.py \\\n",
    "    --model BertModel \\\n",
    "    --model_dir=\"/data/mbert-retrieve-ctx-base/\" \\\n",
    "    --dtype=float32 --log_level=verbose \\\n",
    "    --output_dir \"/data/mbert-retrieve-ctx-base_float16_bert_atten_plugin\" \\\n",
    "    --use_bert_attention_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# becnhmark run onnx model\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        # self.inputs, self.outputs, self.bindings = self.allocate_buffers()\n",
    "        self.stream = cuda.Stream()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self, binding_shape):\n",
    "        # Allocate host and device buffers\n",
    "        inputs, outputs, bindings = [], [], []\n",
    "        for binding in self.engine:\n",
    "            # \n",
    "            # binding_idx = self.engine.get_binding_index(binding) # tensorRT:8.6.1\n",
    "            \n",
    "            # Set input shape based on image dimensions for inference\n",
    "            # conditional: binding is input\n",
    "            # if self.engine.binding_is_input(binding):\n",
    "            #     self.context.set_binding_shape(binding_idx, binding_shape)\n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                self.context.set_input_shape(binding, binding_shape)\n",
    "                \n",
    "            # size = trt.volume(self.context.get_binding_shape(binding_idx))\n",
    "            print(\"binding: \", binding)\n",
    "            size = trt.volume(self.context.get_tensor_shape(binding))\n",
    "            print(\"size: \", size)\n",
    "            print(\"batch_size: \", self.context.get_tensor_shape(binding))\n",
    "            dtype = trt.nptype(self.engine.get_tensor_dtype(binding))\n",
    "            print(\"dtype: \", dtype)\n",
    "\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            # if self.engine.binding_is_input(binding):\n",
    "            if self.engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "\n",
    "        return inputs, outputs, bindings\n",
    "        \n",
    "            \n",
    "    def __call__(self, inputs_id, attention_mask, token_type_ids, batch_size=2):\n",
    "\n",
    "        \n",
    "        x = np.array(inputs_id).astype(self.dtype)\n",
    "        y = np.array(attention_mask).astype(self.dtype)\n",
    "        z = np.array(token_type_ids).astype(self.dtype)\n",
    "\n",
    "\n",
    "        inputs, outputs, bindings = self.allocate_buffers(x.shape)\n",
    "    \n",
    "            \n",
    "        # Transfer input data to the GPU.\n",
    "        # print(x.shape)\n",
    "        np.copyto(inputs[0].host,x.ravel())\n",
    "        np.copyto(inputs[1].host,y.ravel())\n",
    "        np.copyto(inputs[2].host,z.ravel())\n",
    "        \n",
    "        # after copy -> transfer to device, transer first will error duo to hold old value\n",
    "        for inp in inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "\n",
    "        # Run inference\n",
    "        self.context.execute_async_v2(bindings=bindings, stream_handle=self.stream.handle)\n",
    "        \n",
    "        # Transfer prediction output from the GPU.\n",
    "        for out in outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream)\n",
    "        \n",
    "        # Synchronize the stream\n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(batch_size,-1) for out in outputs]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def encode_trt(texts, model, tokenizer, batch_size):\n",
    "    # check if tokenize length is min 128\n",
    "    encoded_input = tokenizer(\n",
    "        texts, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model(\n",
    "        encoded_input['input_ids'],\n",
    "        encoded_input['attention_mask'],\n",
    "        encoded_input['token_type_ids'],\n",
    "        batch_size\n",
    "    )[0]\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # print(embeddings.reshape(batch_size, -1, 768))\n",
    "    return embeddings.reshape(batch_size, -1, 768)[:, 0], end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"./datahub/\")\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]', cache_dir=\"./datahub/\")\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# trt_version 8.6.1\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_dynamic_shape.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"onnx_convert_outputs/mbert-retrieve-qry-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "# trt_engine_ctx_path = \"onnx_convert_outputs/mbert-retrieve-ctx-onnx/model_calib_percential_fp32_int8_dynamic_shape.engine\"\n",
    "\n",
    "# trtversion 10.3.0\n",
    "\n",
    "trt_engine_qry_path = \"./mbert-retrieve-qry-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.engine\"\n",
    "trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float16_tllm_checkpoint/BertModel_float16_tp1_rank0.engine\"\n",
    "\n",
    "# trt_engine_qry_path = \"./mbert-retrieve-qry-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "# trt_engine_ctx_path = \"./mbert-retrieve-ctx-base_float32_tllm_checkpoint/BertModel_float32_tp1_rank0.engine\"\n",
    "\n",
    "model_query = TrtModel(trt_engine_qry_path, max_batch_size=1, dtype=np.int32)\n",
    "model_ctx = TrtModel(trt_engine_ctx_path, max_batch_size=10, dtype=np.int32)\n",
    "tokenizer_qry = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-qry-onnx/\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"onnx_convert_outputs/mbert-retrieve-ctx-onnx/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy_trt(\n",
    "    dataset_eval, \n",
    "    encode_trt,\n",
    "    num_passages=10, \n",
    "    model_ctx=model_ctx,\n",
    "    model_qry=model_query, \n",
    "    tokenizer_ctx=tokenizer_ctx,\n",
    "    tokenizer_query=tokenizer_qry,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run with attention plugin\n",
    "use code run of tensorrtllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# trt llm run.py\n",
    "\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "# isort: off\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "# isort: on\n",
    "\n",
    "import tensorrt_llm\n",
    "from tensorrt_llm import logger\n",
    "from tensorrt_llm.runtime import Session, TensorInfo\n",
    "\n",
    "from build import get_engine_name  # isort:skip\n",
    "\n",
    "\n",
    "def trt_dtype_to_torch(dtype):\n",
    "    if dtype == trt.float16:\n",
    "        return torch.float16\n",
    "    elif dtype == trt.float32:\n",
    "        return torch.float32\n",
    "    elif dtype == trt.int32:\n",
    "        return torch.int32\n",
    "    else:\n",
    "        raise TypeError(\"%s is not supported\" % dtype)\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--log_level', type=str, default='info')\n",
    "    parser.add_argument('--engine_dir', type=str, default='bert_outputs')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_arguments()\n",
    "\n",
    "    tensorrt_llm.logger.set_level(args.log_level)\n",
    "\n",
    "    config_path = os.path.join(args.engine_dir, 'config.json')\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    assert config[\"plugin_config\"][\"remove_input_padding\"] == False, \\\n",
    "        \"Please refer to run_remove_input_padding.py for running BERT models with remove_input_padding enabled\"\n",
    "\n",
    "    dtype = config['builder_config']['precision']\n",
    "    world_size = config['builder_config']['tensor_parallel']\n",
    "    assert world_size == tensorrt_llm.mpi_world_size(), \\\n",
    "        f'Engine world size ({world_size}) != Runtime world size ({tensorrt_llm.mpi_world_size()})'\n",
    "\n",
    "    model_name = config['builder_config']['name']\n",
    "    runtime_rank = tensorrt_llm.mpi_rank() if world_size > 1 else 0\n",
    "\n",
    "    runtime_mapping = tensorrt_llm.Mapping(world_size,\n",
    "                                           runtime_rank,\n",
    "                                           tp_size=world_size)\n",
    "    torch.cuda.set_device(runtime_rank % runtime_mapping.gpus_per_node)\n",
    "\n",
    "    serialize_path = get_engine_name(model_name, dtype, world_size,\n",
    "                                     runtime_rank)\n",
    "    serialize_path = os.path.join(args.engine_dir, serialize_path)\n",
    "\n",
    "    stream = torch.cuda.current_stream().cuda_stream\n",
    "    logger.info(f'Loading engine from {serialize_path}')\n",
    "    with open(serialize_path, 'rb') as f:\n",
    "        engine_buffer = f.read()\n",
    "    logger.info(f'Creating session from engine')\n",
    "    session = Session.from_serialized_engine(engine_buffer)\n",
    "\n",
    "    for i in range(3):\n",
    "        batch_size = (i + 1) * 4\n",
    "        seq_len = (i + 1) * 32\n",
    "        input_ids = torch.randint(100, (batch_size, seq_len)).int().cuda()\n",
    "        input_lengths = seq_len * torch.ones(\n",
    "            (batch_size, ), dtype=torch.int32, device='cuda')\n",
    "        token_type_ids = torch.randint(100, (batch_size, seq_len)).int().cuda()\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'input_lengths': input_lengths,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "        output_info = session.infer_shapes([\n",
    "            TensorInfo('input_ids', trt.DataType.INT32, input_ids.shape),\n",
    "            TensorInfo('input_lengths', trt.DataType.INT32,\n",
    "                       input_lengths.shape),\n",
    "            TensorInfo('token_type_ids', trt.DataType.INT32,\n",
    "                       token_type_ids.shape),\n",
    "        ])\n",
    "        outputs = {\n",
    "            t.name: torch.empty(tuple(t.shape),\n",
    "                                dtype=trt_dtype_to_torch(t.dtype),\n",
    "                                device='cuda')\n",
    "            for t in output_info\n",
    "        }\n",
    "        if (model_name == 'BertModel' or model_name == 'RobertaModel'):\n",
    "            output_name = 'hidden_states'\n",
    "        elif (model_name == 'BertForQuestionAnswering'\n",
    "              or model_name == 'RobertaForQuestionAnswering'):\n",
    "            output_name = 'logits'\n",
    "        elif (model_name == 'BertForSequenceClassification'\n",
    "              or model_name == 'RobertaForSequenceClassification'):\n",
    "            output_name = 'logits'\n",
    "        else:\n",
    "            assert False, f\"Unknown BERT model {model_name}\"\n",
    "\n",
    "        assert output_name in outputs, f'{output_name} not found in outputs, check if build.py set the name correctly'\n",
    "\n",
    "        ok = session.run(inputs, outputs, stream)\n",
    "        assert ok, \"Runtime execution failed\"\n",
    "        torch.cuda.synchronize()\n",
    "        res = outputs[output_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine calibration (API settings dfefault)\n",
    "\n",
    "STATUS: \n",
    "- [Not supported model](https://github.com/NVIDIA/TensorRT-LLM/issues/1614#issuecomment-2122086630) \n",
    "- Config model type không có Bert - [line 110](https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/quantization/quantize_by_modelopt.py#L550)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "query_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-qry-base/')\n",
    "ctx_tokenizer = AutoTokenizer.from_pretrained('mbert-retrieve-ctx-base/')\n",
    "\n",
    "def query_collate_fn(examples):\n",
    "    query = [example['query'] for example in examples]\n",
    "    encoded_input = query_tokenizer(\n",
    "        query, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def ctx_collate_fn(examples):\n",
    "\n",
    "    concate_passage = []\n",
    "    for example in examples:\n",
    "        concate_passage.extend(\n",
    "            [example['positive']] + example['negatives'][:9]\n",
    "        )\n",
    "\n",
    "    # concate_passage = [examples['positive']] + examples['negatives'][:9]\n",
    "    encoded_input = ctx_tokenizer(\n",
    "        concate_passage, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the calibration set and define a forward loop\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import modelopt.torch.quantization as atq\n",
    "\n",
    "batch_size = 4\n",
    "calib_batches = number_samples*2 // batch_size\n",
    "\n",
    "num_workers = 4\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #! in docker with gpus specify, device_id is 0\n",
    "\n",
    "calib_query_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=query_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "calib_ctx_loader = torch.utils.data.DataLoader(\n",
    "    dataset_calib, \n",
    "    batch_size=batch_size,\n",
    "    collate_fn=ctx_collate_fn,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "query_model = AutoModel.from_pretrained('mbert-retrieve-qry-base/', return_dict=True)\n",
    "ctx_model = AutoModel.from_pretrained('mbert-retrieve-ctx-base/', return_dict=True)\n",
    "\n",
    "query_model.to(device)\n",
    "ctx_model.to(device)\n",
    "print(\"Initialize ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def calibrate_loop_query():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_query_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        query_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break\n",
    "\n",
    "def calibrate_loop_ctx():\n",
    "    for i, (encode_input) in tqdm(enumerate(calib_ctx_loader), total=calib_batches):\n",
    "        for k, v in encode_input.items():\n",
    "            encode_input[k] = v.to(device)\n",
    "            # print(k, v.shape)\n",
    "        ctx_model(**encode_input)\n",
    "        if i >= calib_batches:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorrt_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets \n",
    "\n",
    "number_samples = 250 \n",
    "en = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]',\n",
    "                          cache_dir=\"./datahub\")\n",
    "vi = datasets.load_dataset('tiennv/mmarco-passage-vi', split=f'train[:{number_samples}]', cache_dir=\"./datahub\")\n",
    "\n",
    "dataset_calib = concatenate_datasets([en, vi])\n",
    "dataset_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(query_model, config, forward_loop=calibrate_loop_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_tensorrt_llm_checkpoint\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-qry-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model=query_model,  # The quantized model.\n",
    "        decoder_type=\"bert\",\n",
    "        # decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype=dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir=export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select the quantization config, for example, FP8\n",
    "config = atq.FP8_DEFAULT_CFG\n",
    "# PTQ with in-place replacement to quantized modules\n",
    "with torch.no_grad():\n",
    "    atq.quantize(ctx_model, config, forward_loop=calibrate_loop_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ammo.torch.export import export_model_config\n",
    "\n",
    "decoder_type=\"bert\"\n",
    "dtype=torch.float32\n",
    "export_dir=\"./mbert-retrieve-ctx-base-quantize-trtllm-fp8\"\n",
    "# [\"fp8\", \"int8_sq\", \"int4_awq\", \"w4a8_awq\", \"int8_wo\", \"int4_wo\", \"full_prec\"]\n",
    "qformat=\"fp8\"\n",
    "DEFAULT_MAX_SEQ_LEN=512\n",
    "tp_size=1\n",
    "pp_size=1\n",
    "awq_block_size=128\n",
    "# [\"int8\", \"fp8\", None]\n",
    "kv_cache_dtype=\"int8\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_model_config(\n",
    "        ctx_model,  # The quantized model.\n",
    "        decoder_type,  # The type of the model as str, e.g gptj, llama or gptnext.\n",
    "        dtype,  # The exported weights data type as torch.dtype.\n",
    "        export_dir,  # The directory where the exported files will be stored.\n",
    "        inference_tensor_parallel=tp_size,  # The tensor parallelism size for inference.\n",
    "        inference_pipeline_parallel=pp_size,  # The pipeline parallelism size for inference.\n",
    "        export_tensorrt_llm_config=True,  # Enable exporting TensorRT-LLM checkpoint config file.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
