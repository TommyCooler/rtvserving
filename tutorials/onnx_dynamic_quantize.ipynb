{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert onnx model\n",
    "1. query_retrieve\n",
    "2. context_retrieve\n",
    "3. rerank\n",
    "\n",
    "Requirements:\n",
    "1. optinum\n",
    "2. onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../mbert-rerank-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../mbert-rerank-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../mbert-rerank-base')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../mbert-rerank-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task text-classification \\\n",
    "    -m ../mbert-rerank-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-rerank-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task feature-extraction \\\n",
    "    -m ../mbert-retrieve-qry-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-retrieve-qry-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task feature-extraction \\\n",
    "    -m ../mbert-retrieve-ctx-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-retrieve-ctx-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "* `--optimize` ranges from [01, 02, 03, 04]. Increasing value will lead to accuracy dropping.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Dynamic Onnx Model\n",
    "Requirements:\n",
    "* onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "model_fp32 = '../outputs/onnx/mbert-retrieve-qry-onnx/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-retrieve-qry-onnx/model.quant.onnx'\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "model_fp32 = '../outputs/onnx/mbert-retrieve-ctx-onnx/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx'\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "# NOTE: rerank\n",
    "import os\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "# model_fp32 = '../outputs/onnx/mbert-rerank-base-onnx/model.onnx'\n",
    "# model_quant = '../outputs/onnx/mbert-rerank-base--onnx/model.quant.onnx'\n",
    "\n",
    "# use reference\n",
    "model_fp32 = '../outputs/onnx/mbert-rerank-onnx_reference/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-rerank-onnx_reference/model.quant.onnx'\n",
    "\n",
    "\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "\n",
    "extra_options = {'DefaultTensorType': onnx.TensorProto.FLOAT}\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True, extra_options=extra_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# CLS Pooling - Take output from first token\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:,0].detach().cpu()\n",
    "\n",
    "def onnx_predict(onnx_model, encoded_input: dict):\n",
    "    encoded_input = {key: tensor.numpy() for key, tensor in encoded_input.items()}\n",
    "    # Move input to device\n",
    "    start_time = time.time()\n",
    "    model_output = onnx_model.run(None, encoded_input)\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = model_output[0][:, 0] # cls embeddings\n",
    "    return embeddings, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_ctx=None, \n",
    "    model_qry=None, \n",
    "    tokenizer_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    device='cpu'\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        query, time_query = encode_fn([data[i]['query']], model_qry, tokenizer_query, device)\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, device)\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Encode text\n",
    "def encode_onnx(texts, model, tokenizer, device='cpu'):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    #\n",
    "    embeddings, end_time = onnx_predict(model, encoded_input)\n",
    "    #\n",
    "    return embeddings, end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7fccce898bb0>,\n",
       " <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7fcca0017a90>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_query = AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base\")\n",
    "\n",
    "# raw\n",
    "query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model.onnx\"\n",
    "ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model.onnx\"\n",
    "\n",
    "# quant dynamic\n",
    "# query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model.quant.onnx\"\n",
    "# ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx\"\n",
    "\n",
    "# quantize calib\n",
    "# query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/qry_quant_percential_calib.onnx\"\n",
    "# ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/ctx_quant_percential_calib.onnx\"\n",
    "\n",
    "\n",
    "providers = [(\"CUDAExecutionProvider\", {\"device_id\": 1,\n",
    "                                        \"user_compute_stream\": str(torch.cuda.current_stream().cuda_stream),\n",
    "                                        \"cudnn_conv_algo_search\": \"DEFAULT\",\n",
    "                                        })]\n",
    "\n",
    "# providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "# sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel\n",
    "query_session = onnxruntime.InferenceSession(query_path, sess_options, providers=providers)\n",
    "ctx_session = onnxruntime.InferenceSession(ctx_path, sess_options, providers=providers)\n",
    "query_session, ctx_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dummy\n",
    "encode_onnx([dataset_eval[0]['query']], query_session, tokenizer_query)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2433569/3410228632.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
      " 37%|███▋      | 372/1000 [07:31<12:42,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total \u001b[38;5;241m=\u001b[39m \u001b[43meval_accuracy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_passages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Query Run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_query_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36meval_accuracy\u001b[0;34m(data, encode_fn, num_passages, model_ctx, model_qry, tokenizer_ctx, tokenizer_query, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m concate_passage \u001b[38;5;241m=\u001b[39m [data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m+\u001b[39m data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegatives\u001b[39m\u001b[38;5;124m'\u001b[39m][:num_passages\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     50\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 51\u001b[0m encoded_passages, time_ctx \u001b[38;5;241m=\u001b[39m \u001b[43mencode_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcate_passage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     53\u001b[0m time_passage_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end_time\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mencode_onnx\u001b[0;34m(texts, model, tokenizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embeddings, end_time \u001b[38;5;241m=\u001b[39m \u001b[43monnx_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, end_time\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36monnx_predict\u001b[0;34m(onnx_model, encoded_input)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Move input to device\u001b[39;00m\n\u001b[1;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43monnx_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Perform pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/trt-hung/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy(\n",
    "    dataset_eval, \n",
    "    encode_onnx,\n",
    "    num_passages=10, \n",
    "    model_ctx=ctx_session,\n",
    "    model_qry=query_session, \n",
    "    tokenizer_ctx=tokenizer_ctx, \n",
    "    tokenizer_query=tokenizer_query, \n",
    "    device='cpu'\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert tensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `tritonserver` image (experiment in version 24.01 or 24.08)\n",
    "```bash\n",
    "docker run -it --rm -v /home/tiennv/hungnq/rtvserving/outputs/onnx:/onnx --gpus \"device=1\" heronq02/trtllmserve bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context model onnx -> trt model with fp32 & dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context model onnx -> trt model with fp32 & int8 & dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context `quantize dynamic` model onnx -> trt model with fp32 & int8 & dynamic shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[STATUS] - FAIL due to not support operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.quant.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_quant_fp32_int8_dynamic_shape.engine \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_quant_fp32_int8_dynamic_shape.engine \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --int8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trt-hung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
