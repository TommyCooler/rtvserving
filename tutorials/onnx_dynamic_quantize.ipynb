{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert onnx model\n",
    "1. query_retrieve\n",
    "2. context_retrieve\n",
    "3. rerank\n",
    "\n",
    "Requirements:\n",
    "1. optinum\n",
    "2. onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../mbert-rerank-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../mbert-rerank-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../mbert-rerank-base')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../mbert-rerank-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task text-classification \\\n",
    "    -m ../mbert-rerank-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-rerank-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task feature-extraction \\\n",
    "    -m ../mbert-retrieve-qry-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-retrieve-qry-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx \\\n",
    "    --library transformers \\\n",
    "    --task feature-extraction \\\n",
    "    -m ../mbert-retrieve-ctx-base \\\n",
    "    --optimize O1 ../outputs/onnx/mbert-retrieve-ctx-onnx \\\n",
    "    --opset 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "* `--optimize` ranges from [01, 02, 03, 04]. Increasing value will lead to accuracy dropping.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Dynamic Onnx Model\n",
    "Requirements:\n",
    "* onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "model_fp32 = '../outputs/onnx/mbert-retrieve-qry-onnx/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-retrieve-qry-onnx/model.quant.onnx'\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "model_fp32 = '../outputs/onnx/mbert-retrieve-ctx-onnx/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx'\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "# NOTE: rerank\n",
    "import os\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# convert\n",
    "# model_fp32 = '../outputs/onnx/mbert-rerank-base-onnx/model.onnx'\n",
    "# model_quant = '../outputs/onnx/mbert-rerank-base--onnx/model.quant.onnx'\n",
    "\n",
    "# use reference\n",
    "model_fp32 = '../outputs/onnx/mbert-rerank-onnx_reference/model.onnx'\n",
    "model_quant = '../outputs/onnx/mbert-rerank-onnx_reference/model.quant.onnx'\n",
    "\n",
    "\n",
    "if os.path.exists(model_quant):\n",
    "    os.remove(model_quant)\n",
    "    os.remove(model_quant + '.data')\n",
    "\n",
    "extra_options = {'DefaultTensorType': onnx.TensorProto.FLOAT}\n",
    "quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8, use_external_data_format=True, extra_options=extra_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def compute_loss(scores, target):\n",
    "    return cross_entropy(scores, target)\n",
    "\n",
    "def compute_similarity(q_reps, p_reps):\n",
    "    if not isinstance(q_reps, torch.Tensor):\n",
    "        q_reps = torch.tensor(q_reps)\n",
    "    if not isinstance(p_reps, torch.Tensor):\n",
    "        p_reps = torch.tensor(p_reps)\n",
    "    return torch.matmul(q_reps, p_reps.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# CLS Pooling - Take output from first token\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:,0].detach().cpu()\n",
    "\n",
    "def onnx_predict(onnx_model, encoded_input: dict):\n",
    "    # encoded_input = {key: tensor.numpy() for key, tensor in encoded_input.items()}\n",
    "    # Move input to device\n",
    "    start_time = time.time()\n",
    "    model_output = onnx_model.run(None, encoded_input)\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = model_output[0][:, 0] # cls embeddings\n",
    "    return embeddings, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from typing import Callable\n",
    "import inspect\n",
    "\n",
    "def eval_accuracy(\n",
    "    data, \n",
    "    encode_fn = Callable, \n",
    "    num_passages=65, \n",
    "    model_ctx=None, \n",
    "    model_qry=None, \n",
    "    tokenizer_ctx=None, \n",
    "    tokenizer_query=None,\n",
    "    device='cpu'\n",
    "):\n",
    "\n",
    "    assert model_ctx is not None, \"model_ctx is required\"\n",
    "    assert model_qry is not None, \"model_qry is required\"\n",
    "    assert tokenizer_ctx is not None, \"tokenizer_ctx is required\"\n",
    "    assert tokenizer_query is not None, \"tokenizer_query is required\"\n",
    "    assert 'query' in data.column_names, \"data must have query column\"\n",
    "    assert 'positive' in data.column_names, \"data must have positive column\"\n",
    "    assert 'negatives' in data.column_names, \"data must have negatives column\"\n",
    "    # len of arguemtn of encode_fn must be 4\n",
    "    # print(inspect.getargspec(encode_fn).args)\n",
    "    assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        model_ctx = model_ctx.to(device)\n",
    "        model_qry = model_qry.to(device)\n",
    "\n",
    "    time_query_total = 0\n",
    "    time_query_run = 0\n",
    "    time_passage_total = 0\n",
    "    time_passage_run = 0\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        start_time = time.time()\n",
    "        query, time_query = encode_fn([data[i]['query']], model_qry, tokenizer_query, device)\n",
    "        end_time = time.time() - start_time\n",
    "        time_query_total += end_time\n",
    "        time_query_run += time_query\n",
    "\n",
    "        # concate 10 passages\n",
    "        concate_passage = [data[i]['positive']] + data[i]['negatives'][:num_passages-1]\n",
    "        start_time = time.time()\n",
    "        encoded_passages, time_ctx = encode_fn(concate_passage, model_ctx, tokenizer_ctx, device)\n",
    "        end_time = time.time() - start_time\n",
    "        time_passage_total += end_time\n",
    "        time_passage_run += time_ctx\n",
    "\n",
    "        # accuracy\n",
    "        scores = compute_similarity(query, encoded_passages)\n",
    "        if scores.argmax(dim=1).detach().numpy() != 0:\n",
    "            continue\n",
    "        accuracy += 1\n",
    "\n",
    "    return accuracy / len(data), time_query_run/ len(data), time_passage_run/ len(data), time_query_total/ len(data), time_passage_total/ len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "# Encode text\n",
    "def encode_onnx(texts, model, tokenizer, device='cpu'):\n",
    "    # Tokenize sentences\n",
    "    # encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='np')\n",
    "    # for key in encoded_input:\n",
    "    #     assert isinstance(encoded_input[key], np.ndarray), f\"{key} is not numpy array\"\n",
    "    #     print(encoded_input[key].dtype\n",
    "    # print(encoded_input)\n",
    "\n",
    "    encoded_input2 = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input2 = {key: tensor.numpy() for key, tensor in encoded_input2.items()}\n",
    "    # print(encoded_input2)\n",
    "\n",
    "    # for key in encoded_input2:\n",
    "    #     # assert isinstance(encoded_input2[key], np.ndarray), f\"{key} is not numpy array\"\n",
    "    #     print(encoded_input2[key].dtype)\n",
    "    \n",
    "    # Move input to device\n",
    "\n",
    "    #\n",
    "    embeddings, end_time = onnx_predict(model, encoded_input2)\n",
    "    #\n",
    "    return embeddings, end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiennv/.conda/envs/trt-hung/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n",
      "Using the latest cached version of the dataset since tiennv/mmarco-passage-vi couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/tiennv/.cache/huggingface/datasets/tiennv___mmarco-passage-vi/default/0.0.0/5ee2171bc2bc0880d2f35c16063096ec1c4dc4da (last modified on Tue Jan 14 15:38:44 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_id', 'positive', 'negatives'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "en_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "vi_eval = datasets.load_dataset('tiennv/mmarco-passage-vi', split='train[-500:]')\n",
    "\n",
    "dataset_eval = concatenate_datasets([en_eval, vi_eval])\n",
    "dataset_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-02-07 13:44:28.232912653 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-07 13:44:28.239656076 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-07 13:44:28.239670515 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-02-07 13:44:31.569509976 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 12 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-02-07 13:44:31.573973172 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-02-07 13:44:31.573984463 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7f24d6007790>,\n",
       " <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7f26180bac80>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer_query = AutoTokenizer.from_pretrained(\"../mbert-retrieve-qry-base\")\n",
    "tokenizer_ctx = AutoTokenizer.from_pretrained(\"../mbert-retrieve-ctx-base\")\n",
    "\n",
    "# # raw\n",
    "# query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model.onnx\"\n",
    "# ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model.onnx\"\n",
    "\n",
    "# quant dynamic\n",
    "# query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/model.quant.onnx\"\n",
    "# ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx\"\n",
    "\n",
    "# checkpoint from pytorch_quantization_calib.ipynb\n",
    "query_path = \"../outputs/onnx/mbert-retrieve-qry-onnx/qry_quant_percential_calib.onnx\"\n",
    "ctx_path = \"../outputs/onnx/mbert-retrieve-ctx-onnx/ctx_quant_percential_calib.onnx\"\n",
    "\n",
    "\n",
    "\n",
    "providers = [(\"CUDAExecutionProvider\", {\"device_id\": 1,\n",
    "                                        \"user_compute_stream\": str(torch.cuda.current_stream().cuda_stream),\n",
    "                                        \"cudnn_conv_algo_search\": \"DEFAULT\",\n",
    "                                        })]\n",
    "\n",
    "# providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "# sess_options.log_severity_level=1\n",
    "# sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel\n",
    "query_session = onnxruntime.InferenceSession(query_path, sess_options, providers=providers)\n",
    "ctx_session = onnxruntime.InferenceSession(ctx_path, sess_options, providers=providers)\n",
    "query_session, ctx_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dummy\n",
    "encode_onnx([dataset_eval[0]['query']], query_session, tokenizer_query)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672154/3410228632.py:27: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  assert len(inspect.getargspec(encode_fn).args) == 4, \"encode_fn must have 4 arguments\"\n",
      "100%|██████████| 1000/1000 [00:41<00:00, 24.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Time Query Run: 0.003777590751647949\n",
      "Time Passage Run: 0.03482776665687561\n",
      "Time Query Total: 0.004183323621749878\n",
      "Time Passage Total: 0.036229599475860595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy, time_query_run, time_passage_run, time_query_total, time_passage_total = eval_accuracy(\n",
    "    dataset_eval, \n",
    "    encode_onnx,\n",
    "    num_passages=10, \n",
    "    model_ctx=ctx_session,\n",
    "    model_qry=query_session, \n",
    "    tokenizer_ctx=tokenizer_ctx, \n",
    "    tokenizer_query=tokenizer_query, \n",
    "    device='cpu'\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Time Query Run: {time_query_run}\")\n",
    "print(f\"Time Passage Run: {time_passage_run}\")\n",
    "print(f\"Time Query Total: {time_query_total}\")\n",
    "print(f\"Time Passage Total: {time_passage_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert tensorRT - eval in file pytorch_quantization_calib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `tensorrt` image (experiment in version 24.08 with TensorRT 10.3.0)\n",
    "```bash\n",
    "docker run --gpus \"device=1\" -it --rm -v /home/tiennv/hungnq/rtvserving/outputs/onnx:/onnx nvcr.io/nvidia/tensorrt:24.08-py3 bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context model onnx -> trt model with fp32 & dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_fp32_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_fp32_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context model onnx -> trt model with fp32 & int8 & dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_fp32_int8_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_fp32_int8_dynamic_shape.plan \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query & context `quantize dynamic` model onnx -> trt model with fp32 & int8 & dynamic shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[STATUS] - FAIL due to not support operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-qry-onnx/model.quant.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-qry-onnx/model_quant_fp32_int8_dynamic_shape.engine \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:1x256,attention_mask:1x256,token_type_ids:1x256 \\\n",
    "  --maxShapes=input_ids:1x512,attention_mask:1x512,token_type_ids:1x512 \\\n",
    "  --int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "trtexec \\\n",
    "  --onnx=/onnx/mbert-retrieve-ctx-onnx/model.quant.onnx \\\n",
    "  --builderOptimizationLevel=4 \\\n",
    "  --saveEngine=/onnx/mbert-retrieve-ctx-onnx/model_quant_fp32_int8_dynamic_shape.engine \\\n",
    "  --minShapes=input_ids:1x1,attention_mask:1x1,token_type_ids:1x1 \\\n",
    "  --optShapes=input_ids:5x256,attention_mask:5x256,token_type_ids:5x256 \\\n",
    "  --maxShapes=input_ids:10x512,attention_mask:10x512,token_type_ids:10x512 \\\n",
    "  --int8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
